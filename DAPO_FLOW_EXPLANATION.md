# DAPO å®Œæ•´æµç¨‹ï¼šä»ç¯å¢ƒ Reward åˆ°æ¨¡å‹æ›´æ–°

## ğŸ“Š æµç¨‹å›¾æ¦‚è§ˆ

```
ç¯å¢ƒæ‰§è¡Œ â†’ Reward (0/1) â†’ Token-Level Reward â†’ DAPO å¤„ç† â†’ Advantage â†’ Loss â†’ æ¨¡å‹æ›´æ–°
   â†“            â†“                â†“                  â†“            â†“        â†“         â†“
Trajectory   outcome      token_level_scores   Overlong      GRPO    DAPO     Actor
             (scalar)      (sparse tensor)    Shaping      Adv      Clip-    Update
                                                           Calc     Higher
```

---

## ğŸ”„ è¯¦ç»†æµç¨‹æ­¥éª¤

### **Step 1: ç¯å¢ƒæ‰§è¡Œä¸ Reward è·å–**

**ä½ç½®**: `agentevolver/module/trainer/ae_ray_trainer.py:1637`

```python
trajectories = self.env_manager.rollout(tasks, task_exp_configs, mode="sample", ...)
```

- **è¾“å…¥**: Tasks (prompts)
- **è¾“å‡º**: `Trajectory` å¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«ï¼š
  - `traj.reward.outcome`: **Sequence-level reward** (0.0 æˆ– 1.0)
  - `traj.steps`: å¤šè½®å¯¹è¯æ­¥éª¤
  - `traj.is_terminated`: æ˜¯å¦æ­£å¸¸ç»“æŸ

**å…³é”®ç‚¹**: æ­¤æ—¶ reward æ˜¯**æ ‡é‡**ï¼Œè¡¨ç¤ºæ•´ä¸ªè½¨è¿¹çš„æˆåŠŸ/å¤±è´¥ã€‚

---

### **Step 2: Trajectory â†’ DataProto è½¬æ¢**

**ä½ç½®**: `agentevolver/module/trainer/ae_ray_trainer.py:1658`

```python
gen_batch_output = self.env_manager.to_dataproto(all_trajectories)
```

- å°† `Trajectory` è½¬æ¢ä¸º `DataProto`ï¼ŒåŒ…å«ï¼š
  - `batch["prompts"]`, `batch["responses"]`
  - `non_tensor_batch["reward_scores"]`: æ¯ä¸ªæ ·æœ¬çš„ `{"outcome": 0.0/1.0}`

---

### **Step 3: Reward è½¬æ¢ä¸º Token-Level Tensor**

**ä½ç½®**: `agentevolver/module/trainer/ae_ray_trainer.py:1735` â†’ `parse_reward_from_dataproto()`

```python
# agentevolver/module/trainer/ae_ray_trainer.py:73-112
def parse_reward_from_dataproto(data: DataProto, return_dict=False):
    reward_tensor = torch.zeros_like(data.batch["responses"], dtype=torch.float32)  # (bs, seq_len)
    
    # è·å–æ¯ä¸ªæ ·æœ¬çš„ response é•¿åº¦
    response_lengths = attention_masks[:, prompt_lengths:].sum(dim=1)  # (bs,)
    
    # è·å–ç¯å¢ƒè¿”å›çš„ outcome (0.0 æˆ– 1.0)
    reward_scores = torch.tensor([item["outcome"] for item in data.non_tensor_batch["reward_scores"]])
    
    # â­ å…³é”®ï¼šå°† reward æ”¾åœ¨æœ€åä¸€ä¸ªæœ‰æ•ˆ token ä½ç½®ï¼ˆç¨€ç–æ”¾ç½®ï¼‰
    reward_tensor[torch.arange(len(data)), response_lengths - 1] = reward_scores
    
    return reward_tensor  # Shape: (batch_size, response_length)
```

**å…³é”®ç‚¹**:
- Reward æ˜¯**ç¨€ç–çš„**ï¼šåªåœ¨æœ€åä¸€ä¸ªæœ‰æ•ˆ token ä½ç½®æœ‰å€¼ï¼ˆ0.0 æˆ– 1.0ï¼‰
- å…¶ä»–ä½ç½®éƒ½æ˜¯ 0
- Shape: `(batch_size, response_length)`ï¼Œä¾‹å¦‚ `(64, 4096)`

---

### **Step 4: DAPO Overlong Reward Shaping**

**ä½ç½®**: `agentevolver/module/trainer/ae_ray_trainer.py:1921-2020`

```python
# æ£€æµ‹æˆªæ–­æ ·æœ¬
is_truncated_by_length = (actual_response_lengths >= max_response_length - 1)
is_truncated_by_termination = ~traj.is_terminated  # å¤šè½®å¯¹è¯æœªæ­£å¸¸ç»“æŸ
is_truncated = is_truncated_by_length | is_truncated_by_termination

# åº”ç”¨è½¯æƒ©ç½š
reward_tensor = dapo_overlong_reward_shaping(
    rewards=reward_tensor,
    is_truncated=is_truncated,
    truncation_penalty=-0.5,  # é…ç½®ä¸­çš„å€¼
    soft_penalty_mode="additive",
)
```

**`dapo_overlong_reward_shaping` å®ç°** (`het_core_algos.py:572-681`):

```python
# å¯¹äº 2D tensor (batch_size, seq_len)
for i in range(rewards.shape[0]):
    if not is_truncated[i]:
        continue
    
    # è·å–è½¨è¿¹çº§ rewardï¼ˆsum æ‰€æœ‰ tokenï¼‰
    traj_reward = rewards[i].sum()  # ä¾‹å¦‚ï¼š1.0
    
    # åº”ç”¨æƒ©ç½šï¼ˆadditive æ¨¡å¼ï¼‰
    new_traj_reward = traj_reward + truncation_penalty  # 1.0 + (-0.5) = 0.5
    
    # â­ å…³é”®ï¼šåªåœ¨æœ€åä¸€ä¸ªéé›¶ä½ç½®æ”¾ç½®æ–° reward
    reward_pos = non_zero_positions[-1]  # æ‰¾åˆ° reward ä½ç½®
    modified_rewards[i] = 0  # æ¸…ç©ºæ‰€æœ‰ä½ç½®
    modified_rewards[i, reward_pos] = new_traj_reward  # åªåœ¨ä¸€ä¸ªä½ç½®è®¾ç½®
```

**ç»“æœ**:
- æ­£å¸¸æ ·æœ¬ï¼šreward ä¿æŒ 0.0 æˆ– 1.0
- æˆªæ–­æ ·æœ¬ï¼šreward å˜ä¸º -0.5 æˆ– 0.5ï¼ˆ1.0 - 0.5ï¼‰
- **ä»ç„¶ç¨€ç–**ï¼šæ¯ä¸ªæ ·æœ¬åªæœ‰ä¸€ä¸ªéé›¶ä½ç½®

---

### **Step 5: Token-Level Rewards è®¾ç½®**

**ä½ç½®**: `agentevolver/module/trainer/ae_ray_trainer.py:2022-2027`

```python
if self.config.algorithm.use_kl_in_reward:
    # å¦‚æœå¯ç”¨ KL penalty in rewardï¼Œä¼šå‡å»æ¯ä¸ª token çš„ KL divergence
    batch, kl_metrics = apply_kl_penalty(batch, ...)
else:
    # â­ DAPO é…ç½®ä¸­ use_kl_in_reward: falseï¼Œæ‰€ä»¥ç›´æ¥å¤åˆ¶
    batch.batch["token_level_rewards"] = batch.batch["token_level_scores"]
```

**å…³é”®ç‚¹**: 
- `token_level_rewards` = `token_level_scores`ï¼ˆDAPO é…ç½®ä¸‹ï¼‰
- ä»ç„¶æ˜¯ç¨€ç–çš„ï¼šæ¯ä¸ªæ ·æœ¬åªæœ‰ä¸€ä¸ªéé›¶ä½ç½®

---

### **Step 6: DAPO Dynamic Samplingï¼ˆè¿‡æ»¤æ ·æœ¬ï¼‰**

**ä½ç½®**: `agentevolver/module/trainer/ae_ray_trainer.py:2060-2099`

```python
if use_dapo and dapo_config.get("dynamic_sampling", {}).get("enable", False):
    # è·å–åŒä¸€ prompt çš„æ‰€æœ‰ rollouts çš„ rewards
    filter_rewards = batch.batch["token_level_rewards"].sum(dim=-1)  # (batch_size,)
    
    # æŒ‰ group_id (uid) åˆ†ç»„ï¼Œè®¡ç®—æ¯ç»„å‡†ç¡®ç‡
    keep_mask = dapo_filter_samples(
        rewards=filter_rewards,
        group_ids=group_ids,  # åŒä¸€ prompt çš„ rollouts æœ‰ç›¸åŒçš„ uid
        n_rollout=8,
        filter_mode="strict",  # è¿‡æ»¤å…¨å¯¹æˆ–å…¨é”™çš„ç»„
    )
    
    # â­ å…³é”®ï¼šä¸åˆ é™¤æ ·æœ¬ï¼ˆä¼šç ´å GRPO åˆ†ç»„ï¼‰ï¼Œè€Œæ˜¯å°† advantage ç½®é›¶
    batch.batch["dapo_keep_mask"] = keep_mask.float()
```

**`dapo_filter_samples` é€»è¾‘** (`het_core_algos.py:495-569`):

```python
# å¯¹æ¯ä¸ª prompt ç»„ï¼ˆç›¸åŒ uidï¼‰ï¼š
for group_id in unique_groups:
    group_rewards = [rewards[i] for i where uid[i] == group_id]
    accuracy = sum(group_rewards > 0) / len(group_rewards)
    
    if filter_mode == "strict":
        if accuracy == 0.0 or accuracy == 1.0:
            # å…¨é”™æˆ–å…¨å¯¹ â†’ è¿‡æ»¤ï¼ˆæ ‡è®°ä¸º Falseï¼‰
            keep_mask[group_indices] = False
```

**ç»“æœ**:
- è¢«è¿‡æ»¤çš„æ ·æœ¬ï¼šadvantage ä¼šè¢«ç½®é›¶ï¼ˆStep 8ï¼‰
- ä¿æŒ batch ç»“æ„ä¸å˜ï¼ŒGRPO åˆ†ç»„ä»ç„¶æœ‰æ•ˆ

---

### **Step 7: GRPO Advantage è®¡ç®—**

**ä½ç½®**: `agentevolver/module/trainer/ae_ray_trainer.py:2111` â†’ `compute_advantage()` â†’ `compute_grpo_outcome_advantage()`

```python
# agentevolver/module/trainer/ae_ray_trainer.py:158-218
def compute_grpo_outcome_advantage(token_level_rewards, response_mask, index, ...):
    # Step 7.1: è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ€» reward
    scores = token_level_rewards.sum(dim=-1)  # (batch_size,)
    # ä¾‹å¦‚ï¼šscores = [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]  # 8 ä¸ª rollouts
    
    # Step 7.2: æŒ‰ group_id (uid) åˆ†ç»„è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    for i in range(bsz):
        id2score[index[i]].append(scores[i])
    
    for idx in id2score:
        if len(id2score[idx]) > 1:
            id2mean[idx] = torch.mean(torch.tensor(id2score[idx]))  # ç»„å†…å‡å€¼
            id2std[idx] = torch.std(torch.tensor(id2score[idx]))     # ç»„å†…æ ‡å‡†å·®
    
    # Step 7.3: è®¡ç®— normalized advantage
    for i in range(bsz):
        if norm_adv_by_std_in_grpo:
            scores[i] = (scores[i] - id2mean[index[i]]) / (id2std[index[i]] + epsilon)
        else:
            scores[i] = scores[i] - id2mean[index[i]]
    
    # Step 7.4: â­ å…³é”®ï¼šå°† advantage æ‰©å±•åˆ°æ¯ä¸ª token
    scores = scores.unsqueeze(-1) * response_mask  # (batch_size, seq_len)
    # ä¾‹å¦‚ï¼šå¦‚æœ advantage = 0.935ï¼Œresponse æœ‰ 3000 ä¸ª token
    # é‚£ä¹ˆæ¯ä¸ª token çš„ advantage éƒ½æ˜¯ 0.935
    
    return scores, scores  # advantages å’Œ returns ç›¸åŒï¼ˆGRPO ç‰¹æ€§ï¼‰
```

**å…³é”®ç‚¹**:
- Advantage æ˜¯**å¯†é›†çš„**ï¼šæ¯ä¸ªæœ‰æ•ˆ token éƒ½æœ‰ç›¸åŒçš„ advantage å€¼
- Advantage å€¼ = `(reward - group_mean) / group_std`
- èŒƒå›´é€šå¸¸åœ¨ [-2, 2] å·¦å³ï¼ˆå–å†³äºç»„å†…æ–¹å·®ï¼‰

---

### **Step 8: DAPO Dynamic Sampling åº”ç”¨ï¼ˆç½®é›¶ advantageï¼‰**

**ä½ç½®**: `agentevolver/module/trainer/ae_ray_trainer.py:2122-2132`

```python
if "dapo_keep_mask" in batch.batch:
    dapo_keep_mask = batch.batch["dapo_keep_mask"]  # (batch_size,)
    dapo_keep_mask = dapo_keep_mask.unsqueeze(-1)  # (batch_size, 1)
    
    # â­ å°†è¢«è¿‡æ»¤æ ·æœ¬çš„ advantage ç½®é›¶
    batch.batch["advantages"] = batch.batch["advantages"] * dapo_keep_mask
```

**ç»“æœ**: è¢«è¿‡æ»¤æ ·æœ¬çš„ advantage å…¨ä¸º 0ï¼Œä¸ä¼šäº§ç”Ÿæ¢¯åº¦ã€‚

---

### **Step 9: DAPO Policy Loss è®¡ç®—ï¼ˆClip-Higherï¼‰**

**ä½ç½®**: `agentevolver/module/exp_manager/het_actor.py:162-178` â†’ `dapo_compute_policy_loss()`

```python
# agentevolver/module/exp_manager/het_core_algos.py:319-492
def dapo_compute_policy_loss(old_log_prob, log_prob, advantages, response_mask, ...):
    # Step 9.1: è®¡ç®— importance sampling ratio
    ratio = torch.exp(log_prob - old_log_prob)  # Ï€_new / Ï€_old
    
    # Step 9.2: â­ DAPO Clip-Higher æ ¸å¿ƒé€»è¾‘
    # å¯¹äº A > 0ï¼ˆé¼“åŠ±çš„åŠ¨ä½œï¼‰ï¼š
    ratio_clipped_pos = torch.clamp(ratio, 1 - cliprange_low, 1 + cliprange_high)
    # ä¾‹å¦‚ï¼šclip åˆ° [0.8, 1.28]ï¼ˆcliprange_low=0.2, cliprange_high=0.28ï¼‰
    
    # å¯¹äº A < 0ï¼ˆä¸é¼“åŠ±çš„åŠ¨ä½œï¼‰ï¼š
    ratio_clipped_neg = torch.clamp(ratio, 1 - cliprange_low, clip_ratio_c)
    # ä¾‹å¦‚ï¼šclip åˆ° [0.8, 3.0]ï¼ˆç§»é™¤ä¸Šç•Œï¼Œå…è®¸æ›´å¤§çš„å‡å°‘ï¼‰
    
    # Step 9.3: æ ¹æ® advantage ç¬¦å·é€‰æ‹© clipped ratio
    on_pg_losses_clipped = torch.where(
        advantages >= 0,
        -advantages * ratio_clipped_pos,  # A > 0: æ ‡å‡† clip
        -advantages * ratio_clipped_neg,  # A < 0: ç§»é™¤ä¸Šç•Œ
    )
    
    # Step 9.4: PPO-style maxï¼ˆå–æ›´ä¿å®ˆçš„ lossï¼‰
    on_pg_losses = torch.maximum(
        -advantages * ratio,           # æœª clip çš„ loss
        on_pg_losses_clipped           # clip åçš„ loss
    )
    
    # Step 9.5: â­ Token-Level èšåˆï¼ˆDAPO çš„ç¬¬ä¸‰ä¸ªæ”¹è¿›ï¼‰
    pg_loss = agg_loss(
        loss_mat=pg_losses,
        loss_mask=response_mask,
        loss_agg_mode="token-mean",  # æŒ‰ token å¹³å‡ï¼Œè€Œä¸æ˜¯æŒ‰åºåˆ—
    )
    
    return {"pg_loss": pg_loss, ...}
```

**DAPO Clip-Higher çš„å…³é”®**:
- **A > 0**: æ ‡å‡† PPO clip `[1-Îµ_low, 1+Îµ_high]`ï¼Œé™åˆ¶æ¦‚ç‡å¢åŠ 
- **A < 0**: ç§»é™¤ä¸Šç•Œï¼Œå…è®¸ä½æ¦‚ç‡ token è¿›ä¸€æ­¥å‡å°‘ï¼Œ**é˜²æ­¢ç†µå´©å¡Œ**

**Token-Level èšåˆ**:
- `loss_agg_mode: token-mean` ç¡®ä¿æ¯ä¸ª token å¯¹ loss çš„è´¡çŒ®ç›¸ç­‰
- é¿å…çŸ­åºåˆ—è¢«è¿‡åº¦åŠ æƒ

---

### **Step 10: å®Œæ•´ Loss è®¡ç®—ä¸æ¨¡å‹æ›´æ–°**

**ä½ç½®**: `agentevolver/module/exp_manager/het_actor.py:62-220`

```python
# Step 10.1: è®¡ç®—å®Œæ•´ loss
total_loss = (
    pg_loss +                    # DAPO policy loss
    kl_loss_coef * kl_loss +    # KL divergence lossï¼ˆå¦‚æœå¯ç”¨ï¼‰
    entropy_coeff * entropy_loss # Entropy bonusï¼ˆé€šå¸¸ä¸º 0ï¼‰
)

# Step 10.2: åå‘ä¼ æ’­
total_loss.backward()

# Step 10.3: æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(self.actor_module.parameters(), grad_clip)

# Step 10.4: ä¼˜åŒ–å™¨æ›´æ–°
optimizer.step()
```

---

## ğŸ“ˆ æ•°æ®æµå½¢çŠ¶å˜åŒ–æ€»ç»“

| æ­¥éª¤ | æ•°æ® | Shape | ç¨€ç–æ€§ | è¯´æ˜ |
|------|------|-------|--------|------|
| 1. ç¯å¢ƒ | `outcome` | `(1,)` | - | æ ‡é‡ï¼š0.0 æˆ– 1.0 |
| 2. Token-Level | `token_level_scores` | `(bs, seq_len)` | âœ… ç¨€ç– | åªåœ¨æœ€åä¸€ä¸ª token æœ‰å€¼ |
| 3. Overlong Shaping | `token_level_scores` | `(bs, seq_len)` | âœ… ç¨€ç– | æˆªæ–­æ ·æœ¬ï¼šreward - 0.5 |
| 4. Token Rewards | `token_level_rewards` | `(bs, seq_len)` | âœ… ç¨€ç– | = token_level_scores |
| 5. Advantage | `advantages` | `(bs, seq_len)` | âŒ å¯†é›† | æ¯ä¸ªæœ‰æ•ˆ token éƒ½æœ‰å€¼ |
| 6. Policy Loss | `pg_losses` | `(bs, seq_len)` | âŒ å¯†é›† | æ¯ä¸ª token çš„ loss |
| 7. Aggregated Loss | `pg_loss` | `(1,)` | - | Token-level å¹³å‡ |

---

## ğŸ¯ DAPO çš„ä¸‰ä¸ªæ ¸å¿ƒæ”¹è¿›

### 1. **Clip-Higherï¼ˆè§£è€¦éå¯¹ç§°è£å‰ªï¼‰**
- **ä½ç½®**: `dapo_compute_policy_loss()` (Step 9)
- **æ•ˆæœ**: A < 0 æ—¶ç§»é™¤ä¸Šç•Œï¼Œé˜²æ­¢ç†µå´©å¡Œ

### 2. **Dynamic Samplingï¼ˆåŠ¨æ€é‡‡æ ·ï¼‰**
- **ä½ç½®**: `dapo_filter_samples()` (Step 6)
- **æ•ˆæœ**: è¿‡æ»¤å…¨å¯¹/å…¨é”™çš„ prompt ç»„ï¼Œå‡å°‘æ— æ•ˆæ¢¯åº¦

### 3. **Token-Level Policy Gradient**
- **ä½ç½®**: `loss_agg_mode: token-mean` (Step 9.5)
- **æ•ˆæœ**: æŒ‰ token å¹³å‡ lossï¼Œé¿å…çŸ­åºåˆ—è¢«è¿‡åº¦åŠ æƒ

### 4. **Overlong Reward Shapingï¼ˆæˆªæ–­å¥–åŠ±å¡‘é€ ï¼‰**
- **ä½ç½®**: `dapo_overlong_reward_shaping()` (Step 4)
- **æ•ˆæœ**: å¯¹æˆªæ–­æ ·æœ¬åº”ç”¨è½¯æƒ©ç½šï¼ˆ-0.5ï¼‰ï¼Œä¿ç•™éƒ¨åˆ†å­¦ä¹ ä¿¡å·

---

## ğŸ” å…³é”®ä»£ç ä½ç½®ç´¢å¼•

| åŠŸèƒ½ | æ–‡ä»¶ | è¡Œå· |
|------|------|------|
| ç¯å¢ƒæ‰§è¡Œ | `ae_ray_trainer.py` | 1637 |
| Reward è½¬æ¢ | `ae_ray_trainer.py` | 73-112 |
| Overlong Shaping | `ae_ray_trainer.py` | 1921-2020 |
| Overlong å®ç° | `het_core_algos.py` | 572-681 |
| Dynamic Sampling | `ae_ray_trainer.py` | 2060-2099 |
| Dynamic å®ç° | `het_core_algos.py` | 495-569 |
| GRPO Advantage | `ae_ray_trainer.py` | 158-218 |
| DAPO Loss | `het_core_algos.py` | 319-492 |
| æ¨¡å‹æ›´æ–° | `het_actor.py` | 62-220 |

---

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆ `critic/rewards_last/mean` å’Œ `critic/rewards_sum/mean` ä¸åŒï¼Ÿ

**A**: 
- `rewards_sum` = `token_level_rewards.sum(-1)`ï¼šæ¯ä¸ªæ ·æœ¬çš„æ€» reward
- `rewards_last` = `token_level_rewards[batch_idx, last_resp_idx]`ï¼šæœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„ reward

å¦‚æœä¸åŒï¼Œå¯èƒ½åŸå› ï¼š
1. **Token-level shaping**ï¼šæŸäº› token æœ‰é¢å¤–çš„ penaltyï¼ˆä½† DAPO é…ç½®ä¸‹ä¸åº”è¯¥ï¼‰
2. **ä½ç½®ä¸ä¸€è‡´**ï¼šreward å®é™…ä½ç½® â‰  metrics è®¤ä¸ºçš„æœ€åä¸€ä¸ª tokenï¼ˆå¤šè½®å¯¹è¯å¸¸è§ï¼‰

### Q2: Advantage ä¸ºä»€ä¹ˆæ˜¯å¯†é›†çš„ï¼Ÿ

**A**: GRPO å°† sequence-level advantage æ‰©å±•åˆ°æ¯ä¸ª tokenï¼Œè¿™æ ·æ¯ä¸ª token éƒ½èƒ½è·å¾—ç›¸åŒçš„å­¦ä¹ ä¿¡å·ã€‚è¿™æ˜¯ GRPO çš„è®¾è®¡ï¼Œä¸æ˜¯ bugã€‚

### Q3: ä¸ºä»€ä¹ˆ validation çš„ reward æ­£å¸¸ï¼ˆ0-1ï¼‰ï¼Œä½†è®­ç»ƒæ—¶ä¼šæœ‰è´Ÿæ•°ï¼Ÿ

**A**: Validation ä¸ç»è¿‡ `dapo_overlong_reward_shaping`ï¼Œæ‰€ä»¥ reward ä¿æŒåŸå§‹çš„ 0.0/1.0ã€‚è®­ç»ƒæ—¶æˆªæ–­æ ·æœ¬ä¼šè¢«å‡å» 0.5ï¼Œæ‰€ä»¥å¯èƒ½å˜æˆ -0.5 æˆ– 0.5ã€‚

