# DAPO ä¸­ Off-Policy Data (Experience Replay) çš„å¤„ç†æœºåˆ¶

## ğŸ“‹ æ¦‚è¿°

DAPO é€šè¿‡ `exp_mask` åŒºåˆ† on-policy å’Œ off-policy æ•°æ®ï¼Œå¹¶åº”ç”¨ä¸åŒçš„ loss è®¡ç®—ç­–ç•¥ï¼š
- **On-policy**: ä½¿ç”¨ DAPO çš„ **Clip-Higher** æœºåˆ¶
- **Off-policy**: ä½¿ç”¨ **ExGRPO Policy Shaping** å¤„ç†é‡è¦æ€§é‡‡æ ·

---

## ğŸ” å®ç°ä½ç½®

### **1. Exp Mask åˆ›å»º**

**æ–‡ä»¶**: `agentevolver/module/env_manager/env_manager.py:634-653`

```python
# Create experience mask
if sample.extras.get("is_experience_replay", False):
    # Experience Replay: åªå¯¹ LLM å“åº”ä½ç½®ï¼ˆloss_mask=1ï¼‰è®¾ç½® exp_mask=1
    prompt_exp_mask_list.append(torch.zeros(len(sample.prompt_loss_mask), dtype=torch.int))
    response_exp_mask_list.append(torch.tensor(sample.response_loss_mask, dtype=torch.int))
else:
    # On-policy: å…¨ä¸º 0
    prompt_exp_mask_list.append(torch.zeros(len(sample.prompt_loss_mask), dtype=torch.int))
    response_exp_mask_list.append(torch.zeros(len(sample.response_loss_mask), dtype=torch.int))
```

**å…³é”®ç‚¹**:
- `exp_mask = 1`: Off-policy dataï¼ˆæ¥è‡ª Experience Replayï¼‰
- `exp_mask = 0`: On-policy dataï¼ˆå½“å‰ç­–ç•¥ç”Ÿæˆï¼‰
- **Multi-turn å…³é”®**: åªå¯¹ LLM å“åº”ä½ç½®ï¼ˆ`loss_mask=1`ï¼‰è®¾ç½® `exp_mask=1`ï¼ŒEnvironment å“åº”ä¸å‚ä¸ off-policy loss

---

### **2. DAPO Loss è®¡ç®—ä¸­çš„ Off-Policy å¤„ç†**

**æ–‡ä»¶**: `agentevolver/module/exp_manager/het_core_algos.py:428-457`

```python
def dapo_compute_policy_loss(
    old_log_prob, log_prob, advantages, response_mask,
    exp_mask=None,  # â­ Off-policy mask
    off_policy_shaping_mode="exgrpo_policy_shaping",
    off_policy_shaping_beta=0.1,
    ...
):
    # Step 1: è®¡ç®— importance sampling ratio
    ratio = torch.exp(log_prob - old_log_prob)  # Ï€_new / Ï€_old
    
    # Step 2: å¤„ç† exp_mask
    if exp_mask is None:
        exp_mask = torch.zeros_like(response_mask)
    exp_mask = exp_mask.float()
    
    # Step 3: â­ On-policy loss (DAPO Clip-Higher)
    # ... ä½¿ç”¨ DAPO Clip-Higher æœºåˆ¶ ...
    on_policy_mask = (1.0 - exp_mask) * response_mask
    on_pg_loss = verl_F.masked_mean(on_pg_losses, on_policy_mask)
    
    # Step 4: â­ Off-policy loss (ExGRPO Policy Shaping)
    if off_policy_shaping_mode == "exgrpo_policy_shaping":
        # ExGRPO Policy Shaping: f(x) = x / (x + Î²)
        off_ratio = ratio  # ä½¿ç”¨ç›¸åŒçš„ importance sampling ratio
        off_ratio_shaped = off_ratio / (off_ratio + off_policy_shaping_beta)
        off_pg_losses = -advantages * off_ratio_shaped
    elif off_policy_shaping_mode == "dapo_clip_higher":
        # å¯¹ off-policy ä¹Ÿä½¿ç”¨ DAPO Clip-Higher
        off_pg_losses = on_pg_losses
    else:
        raise ValueError(f"Invalid off_policy_shaping_mode")
    
    # Step 5: è®¡ç®— off-policy loss
    off_policy_mask = exp_mask * response_mask
    off_pg_loss = verl_F.masked_mean(off_pg_losses, off_policy_mask)
    
    # Step 6: â­ åˆå¹¶ on-policy å’Œ off-policy losses
    pg_losses = off_pg_losses * exp_mask + on_pg_losses * (1.0 - exp_mask)
    pg_loss = agg_loss(loss_mat=pg_losses, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)
```

---

## ğŸ¯ ä¸¤ç§ Off-Policy å¤„ç†æ¨¡å¼

### **æ¨¡å¼ 1: ExGRPO Policy Shapingï¼ˆæ¨èï¼Œé»˜è®¤ï¼‰**

**å…¬å¼**: `f(x) = x / (x + Î²)`

**å®ç°**:
```python
off_ratio = ratio  # importance sampling ratio: Ï€_new / Ï€_old
off_ratio_shaped = off_ratio / (off_ratio + off_policy_shaping_beta)  # f(x) = x/(x+Î²)
off_pg_losses = -advantages * off_ratio_shaped
```

**ç‰¹ç‚¹**:
- âœ… **æ”¾å¤§ä½æ¦‚ç‡ä¿¡å·**: å½“ `ratio` å¾ˆå°æ—¶ï¼ˆä¾‹å¦‚ 0.1ï¼‰ï¼Œ`f(0.1) = 0.1/(0.1+0.1) = 0.5`ï¼Œæ”¾å¤§äº† 5 å€
- âœ… **æŠ‘åˆ¶é«˜æ¦‚ç‡ä¿¡å·**: å½“ `ratio` å¾ˆå¤§æ—¶ï¼ˆä¾‹å¦‚ 10ï¼‰ï¼Œ`f(10) = 10/(10+0.1) â‰ˆ 0.99`ï¼Œå‡ ä¹ä¸å˜
- âœ… **ä¿æŒç†µ**: é€šè¿‡æ”¾å¤§ä½æ¦‚ç‡ä¿¡å·ï¼Œé¼“åŠ±æ¨¡å‹ä¿æŒæ¢ç´¢
- âœ… **ä¸éœ€è¦ clipping**: å‡½æ•°æœ¬èº«å°±æœ‰ç•Œ `[0, 1)`ï¼Œæ›´å¹³æ»‘

**æ•°å­¦æ€§è´¨**:
```
f(x) = x / (x + Î²)

å½“ x â†’ 0:  f(x) â†’ 0
å½“ x â†’ +âˆ: f(x) â†’ 1
å½“ x = Î²:  f(x) = 0.5  (æ‹ç‚¹)
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**:
- Off-policy æ•°æ®æ¥è‡ªå†å²ç­–ç•¥ï¼Œåˆ†å¸ƒå¯èƒ½å·²ç»åç§»
- ç›´æ¥ä½¿ç”¨ importance sampling ratio å¯èƒ½å¯¼è‡´é«˜æ–¹å·®
- ExGRPO shaping é€šè¿‡éçº¿æ€§å˜æ¢ç¨³å®šæ¢¯åº¦ï¼ŒåŒæ—¶ä¿æŒæ¢ç´¢æ€§

---

### **æ¨¡å¼ 2: DAPO Clip-Higherï¼ˆå®éªŒæ€§ï¼‰**

**å®ç°**:
```python
off_pg_losses = on_pg_losses  # ç›´æ¥ä½¿ç”¨ on-policy çš„ DAPO Clip-Higher loss
```

**ç‰¹ç‚¹**:
- âš ï¸ **å®éªŒæ€§**: å¯¹ off-policy ä¹Ÿåº”ç”¨ DAPO Clip-Higher
- âš ï¸ **å¯èƒ½ä¸ç¨³å®š**: Off-policy æ•°æ®çš„åˆ†å¸ƒåç§»å¯èƒ½å¯¼è‡´ Clip-Higher æ•ˆæœä¸ä½³
- âš ï¸ **ä¸æ¨è**: é»˜è®¤ä½¿ç”¨ ExGRPO shaping æ›´ç¨³å®š

---

## âœ… å®ç°æ­£ç¡®æ€§åˆ†æ

### **1. Importance Sampling Ratio è®¡ç®—**

```python
ratio = torch.exp(log_prob - old_log_prob)  # Ï€_new / Ï€_old
```

**æ­£ç¡®æ€§**: âœ… **æ­£ç¡®**
- å¯¹äº **on-policy** æ•°æ®: `old_log_prob` æ˜¯å½“å‰ç­–ç•¥çš„ log_probï¼Œæ‰€ä»¥ `ratio â‰ˆ 1.0`
- å¯¹äº **off-policy** æ•°æ®: `old_log_prob` æ˜¯å†å²ç­–ç•¥çš„ log_probï¼ˆä» Experience Replay åŠ è½½ï¼‰ï¼Œæ‰€ä»¥ `ratio` åæ˜ ç­–ç•¥å˜åŒ–

**å…³é”®**: Off-policy æ•°æ®å¿…é¡»ä½¿ç”¨**å†å²ç­–ç•¥çš„ old_log_prob**ï¼Œè¿™åœ¨ Experience Replay æµç¨‹ä¸­å·²æ­£ç¡®å¤„ç†ã€‚

---

### **2. ExGRPO Policy Shaping å…¬å¼**

```python
off_ratio_shaped = off_ratio / (off_ratio + off_policy_shaping_beta)
```

**æ­£ç¡®æ€§**: âœ… **æ­£ç¡®**
- å…¬å¼ä¸ ExGRPO è®ºæ–‡ä¸€è‡´: `f(w*(Î¸)) = w*(Î¸) / (w*(Î¸) + Î²)`
- å…¶ä¸­ `w*(Î¸) = exp(log_prob - old_log_prob)` æ˜¯ importance sampling ratio

**éªŒè¯**:
- å½“ `off_ratio = 0.1`, `beta = 0.1`: `f(0.1) = 0.1/(0.1+0.1) = 0.5` âœ…
- å½“ `off_ratio = 1.0`, `beta = 0.1`: `f(1.0) = 1.0/(1.0+0.1) â‰ˆ 0.909` âœ…
- å½“ `off_ratio = 10.0`, `beta = 0.1`: `f(10.0) = 10.0/(10.0+0.1) â‰ˆ 0.990` âœ…

---

### **3. Loss åˆå¹¶**

```python
pg_losses = off_pg_losses * exp_mask + on_pg_losses * (1.0 - exp_mask)
pg_loss = agg_loss(loss_mat=pg_losses, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)
```

**æ­£ç¡®æ€§**: âœ… **æ­£ç¡®**
- ä½¿ç”¨ `exp_mask` è¿›è¡Œå…ƒç´ çº§åˆ«çš„æ··åˆ
- å¯¹äº `exp_mask=1` çš„ä½ç½®ï¼Œä½¿ç”¨ `off_pg_losses`
- å¯¹äº `exp_mask=0` çš„ä½ç½®ï¼Œä½¿ç”¨ `on_pg_losses`
- æœ€åä½¿ç”¨ `token-mean` èšåˆï¼ˆDAPO çš„ç¬¬ä¸‰ä¸ªæ”¹è¿›ï¼‰

---

### **4. Mask ä½¿ç”¨**

```python
on_policy_mask = (1.0 - exp_mask) * response_mask
off_policy_mask = exp_mask * response_mask
```

**æ­£ç¡®æ€§**: âœ… **æ­£ç¡®**
- ç¡®ä¿åªå¯¹æœ‰æ•ˆ tokenï¼ˆ`response_mask=1`ï¼‰è®¡ç®— loss
- æ­£ç¡®åŒºåˆ† on-policy å’Œ off-policy æ•°æ®
- Multi-turn åœºæ™¯ä¸‹ï¼Œåªå¯¹ LLM å“åº”éƒ¨åˆ†ï¼ˆ`loss_mask=1`ï¼‰è®¾ç½® `exp_mask=1`

---

## ğŸ”¬ æ½œåœ¨é—®é¢˜ä¸æ”¹è¿›å»ºè®®

### **é—®é¢˜ 1: Off-Policy Advantage è®¡ç®—**

**å½“å‰å®ç°**: Off-policy æ•°æ®ä½¿ç”¨ä¸ on-policy ç›¸åŒçš„ advantageï¼ˆæ¥è‡ª GRPO åˆ†ç»„è®¡ç®—ï¼‰

**æ½œåœ¨é—®é¢˜**:
- Off-policy æ•°æ®æ¥è‡ªå†å²ç­–ç•¥ï¼Œadvantage å¯èƒ½ä¸å‡†ç¡®
- å¦‚æœ off-policy å’Œ on-policy æ•°æ®åœ¨åŒä¸€ä¸ª GRPO ç»„ä¸­ï¼Œadvantage æ˜¯æ··åˆè®¡ç®—çš„

**åˆ†æ**:
- âœ… **å½“å‰å®ç°æ˜¯åˆç†çš„**: ExGRPO è®ºæ–‡ä¸­ï¼Œoff-policy å’Œ on-policy æ•°æ®å…±äº«ç›¸åŒçš„ advantageï¼ˆåŸºäºåŒä¸€ task çš„ outcomeï¼‰
- âœ… **Policy shaping å·²ç»å¤„ç†äº†åˆ†å¸ƒåç§»**: `f(x) = x/(x+Î²)` é€šè¿‡è°ƒæ•´ importance sampling ratio æ¥è¡¥å¿åˆ†å¸ƒåç§»

---

### **é—®é¢˜ 2: Beta å‚æ•°é€‰æ‹©**

**å½“å‰é…ç½®**: `off_policy_shaping_beta: 0.1`

**åˆ†æ**:
- âœ… **0.1 æ˜¯ ExGRPO è®ºæ–‡çš„é»˜è®¤å€¼**: ç»è¿‡å®éªŒéªŒè¯
- âš ï¸ **å¯èƒ½éœ€è¦è°ƒä¼˜**: å¦‚æœ off-policy æ•°æ®æ¯”ä¾‹å¾ˆé«˜ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ beta
- ğŸ’¡ **å»ºè®®**: ç›‘æ§ `actor/off_pg_loss` å’Œ `actor/on_pg_loss` çš„æ¯”ä¾‹ï¼Œå¦‚æœ off-policy loss è¿‡å¤§ï¼Œå¯ä»¥å¢å¤§ beta

---

### **é—®é¢˜ 3: Off-Policy æ•°æ®çš„ Clip-Higher é€‰é¡¹**

**å½“å‰å®ç°**: æä¾›äº† `dapo_clip_higher` é€‰é¡¹ï¼Œä½†å¯¹ off-policy åº”ç”¨ Clip-Higher å¯èƒ½ä¸ç¨³å®š

**åˆ†æ**:
- âš ï¸ **ç†è®ºä¸Šå¯èƒ½æœ‰é—®é¢˜**: Off-policy æ•°æ®çš„åˆ†å¸ƒå·²ç»åç§»ï¼ŒClip-Higher çš„è®¾è®¡å‡è®¾å¯èƒ½ä¸æˆç«‹
- âœ… **é»˜è®¤ä½¿ç”¨ ExGRPO shaping æ˜¯å®‰å…¨çš„**: è¿™æ˜¯ç»è¿‡éªŒè¯çš„æ–¹æ³•
- ğŸ’¡ **å»ºè®®**: é™¤éæœ‰ç‰¹æ®Šéœ€æ±‚ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤çš„ `exgrpo_policy_shaping`

---

## ğŸ“Š å®Œæ•´æµç¨‹ç¤ºä¾‹

### **åœºæ™¯**: Batch ä¸­æœ‰ 64 ä¸ªæ ·æœ¬ï¼Œå…¶ä¸­ 32 ä¸ªæ˜¯ off-policy

```python
# è¾“å…¥
old_log_prob: (64, 4096)  # å¯¹äº off-policyï¼Œè¿™æ˜¯å†å²ç­–ç•¥çš„ log_prob
log_prob: (64, 4096)       # å½“å‰ç­–ç•¥çš„ log_prob
advantages: (64, 4096)     # GRPO è®¡ç®—çš„ advantageï¼ˆon/off-policy å…±äº«ï¼‰
response_mask: (64, 4096)  # æœ‰æ•ˆ token mask
exp_mask: (64, 4096)       # [0,0,...,0, 1,1,...,1] å‰ 32 ä¸ªæ˜¯ on-policyï¼Œå 32 ä¸ªæ˜¯ off-policy

# Step 1: è®¡ç®— ratio
ratio = exp(log_prob - old_log_prob)  # (64, 4096)
# å¯¹äº on-policy: ratio â‰ˆ 1.0
# å¯¹äº off-policy: ratio å¯èƒ½åç¦» 1.0ï¼ˆåæ˜ ç­–ç•¥å˜åŒ–ï¼‰

# Step 2: On-policy loss (DAPO Clip-Higher)
on_pg_losses = compute_dapo_clip_higher_loss(ratio, advantages, ...)  # (64, 4096)
on_policy_mask = (1.0 - exp_mask) * response_mask  # å‰ 32 ä¸ªæ ·æœ¬
on_pg_loss = masked_mean(on_pg_losses, on_policy_mask)  # scalar

# Step 3: Off-policy loss (ExGRPO Shaping)
off_ratio_shaped = ratio / (ratio + 0.1)  # (64, 4096)
off_pg_losses = -advantages * off_ratio_shaped  # (64, 4096)
off_policy_mask = exp_mask * response_mask  # å 32 ä¸ªæ ·æœ¬
off_pg_loss = masked_mean(off_pg_losses, off_policy_mask)  # scalar

# Step 4: åˆå¹¶
pg_losses = off_pg_losses * exp_mask + on_pg_losses * (1.0 - exp_mask)  # (64, 4096)
pg_loss = agg_loss(pg_losses, response_mask, "token-mean")  # scalar

# Step 5: åå‘ä¼ æ’­
pg_loss.backward()
```

---

## âœ… æ€»ç»“

### **å®ç°æ­£ç¡®æ€§**: âœ… **åŸºæœ¬æ­£ç¡®**

1. âœ… **Importance Sampling Ratio**: æ­£ç¡®è®¡ç®—ï¼Œoff-policy ä½¿ç”¨å†å²ç­–ç•¥çš„ old_log_prob
2. âœ… **ExGRPO Policy Shaping**: å…¬å¼æ­£ç¡®ï¼Œä¸è®ºæ–‡ä¸€è‡´
3. âœ… **Loss åˆå¹¶**: ä½¿ç”¨ exp_mask æ­£ç¡®æ··åˆ
4. âœ… **Mask ä½¿ç”¨**: æ­£ç¡®å¤„ç† multi-turn åœºæ™¯

### **è®¾è®¡åˆç†æ€§**: âœ… **åˆç†**

1. âœ… **åˆ†ç¦»å¤„ç†**: On-policy ç”¨ DAPO Clip-Higherï¼Œoff-policy ç”¨ ExGRPO shapingï¼Œå„å¸å…¶èŒ
2. âœ… **å…¼å®¹æ€§**: ä¸åŸæœ‰çš„ Experience-Replay + GRPO æœºåˆ¶å…¼å®¹
3. âœ… **ç¨³å®šæ€§**: ExGRPO shaping æ¯”ç›´æ¥åº”ç”¨ Clip-Higher æ›´ç¨³å®š

### **å»ºè®®**

1. âœ… **ä¿æŒå½“å‰å®ç°**: é»˜è®¤ä½¿ç”¨ `exgrpo_policy_shaping` æ˜¯æ­£ç¡®çš„é€‰æ‹©
2. ğŸ’¡ **ç›‘æ§æŒ‡æ ‡**: å…³æ³¨ `actor/off_pg_loss` å’Œ `actor/on_pg_loss` çš„æ¯”ä¾‹
3. ğŸ’¡ **Beta è°ƒä¼˜**: å¦‚æœ off-policy loss è¿‡å¤§ï¼Œå¯ä»¥å°è¯•å¢å¤§ `off_policy_shaping_beta`
4. âš ï¸ **é¿å…ä½¿ç”¨ `dapo_clip_higher`**: é™¤éæœ‰ç‰¹æ®Šå®éªŒéœ€æ±‚ï¼Œå¦åˆ™ä¸æ¨è

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

- **ExGRPO è®ºæ–‡**: Experience-Guided Reinforcement Learning with Shared Representations
- **DAPO è®ºæ–‡**: Decoupled Clip and Dynamic sAmpling Policy Optimization
- **å®ç°ä½ç½®**: `agentevolver/module/exp_manager/het_core_algos.py:319-492`

