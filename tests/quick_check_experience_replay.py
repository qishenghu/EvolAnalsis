#!/usr/bin/env python3
"""
Experience Replay å¿«é€Ÿæ£€æŸ¥è„šæœ¬

è¿™ä¸ªè„šæœ¬å¯ä»¥å¿«é€Ÿæ£€æŸ¥ experience replay çš„å…³é”®ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
ä¸éœ€è¦å®Œæ•´çš„ç¯å¢ƒï¼Œåªéœ€è¦åŸºæœ¬çš„ Python å’Œ torchã€‚

ä½¿ç”¨æ–¹æ³•:
    python tests/quick_check_experience_replay.py
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def check_imports():
    """æ£€æŸ¥å¿…è¦çš„å¯¼å…¥"""
    print("="*80)
    print("æ£€æŸ¥ 1: å¯¼å…¥æ£€æŸ¥")
    print("="*80)
    
    try:
        import torch
        print("âœ“ torch å¯ç”¨")
    except ImportError:
        print("âŒ torch ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install torch")
        return False
    
    try:
        from agentevolver.module.exp_manager.het_core_algos import het_compute_token_on_off_policy_loss
        print("âœ“ het_compute_token_on_off_policy_loss å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from agentevolver.module.exp_manager.exp_manager import ExperienceManager
        print("âœ“ ExperienceManager å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from agentevolver.module.exp_manager.experience_collate import ExperienceMixCollateFn
        print("âœ“ ExperienceMixCollateFn å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True


def check_loss_computation():
    """æ£€æŸ¥ loss è®¡ç®—å‡½æ•°"""
    print("\n" + "="*80)
    print("æ£€æŸ¥ 2: Loss è®¡ç®—å‡½æ•°")
    print("="*80)
    
    try:
        import torch
        from agentevolver.module.exp_manager.het_core_algos import het_compute_token_on_off_policy_loss
        import verl.utils.torch_functional as verl_F
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, response_len = 4, 10
    old_log_prob = torch.randn(batch_size, response_len)
    log_prob = torch.randn(batch_size, response_len)
    advantages = torch.randn(batch_size, response_len)
    response_mask = torch.ones(batch_size, response_len)
    exp_mask = torch.zeros(batch_size, response_len)
    exp_mask[0, :] = 1  # ç¬¬ä¸€ä¸ªæ ·æœ¬æ˜¯ off-policy
    
    # æµ‹è¯• higher_clip_bound
    try:
        result1 = het_compute_token_on_off_policy_loss(
            old_log_prob=old_log_prob,
            log_prob=log_prob,
            advantages=advantages,
            response_mask=response_mask,
            exp_mask=exp_mask,
            cliprange=0.2,
            cliprange_low=0.2,
            cliprange_high=0.2,
            off_cliprange_high=0.6,
            clip_ratio_c=3.0,
            loss_agg_mode="token-mean",
            off_policy_shaping_mode="higher_clip_bound",
            off_policy_shaping_beta=0.1,
        )
        print("âœ“ higher_clip_bound æ–¹å¼è®¡ç®—æˆåŠŸ")
        print(f"  - pg_loss: {result1['pg_loss'].item():.4f}")
        print(f"  - on_pg_loss: {result1['on_pg_loss'].item():.4f}")
        print(f"  - off_pg_loss: {result1['off_pg_loss'].item():.4f}")
    except Exception as e:
        print(f"âŒ higher_clip_bound è®¡ç®—å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯• exgrpo_policy_shaping
    try:
        result2 = het_compute_token_on_off_policy_loss(
            old_log_prob=old_log_prob,
            log_prob=log_prob,
            advantages=advantages,
            response_mask=response_mask,
            exp_mask=exp_mask,
            cliprange=0.2,
            cliprange_low=0.2,
            cliprange_high=0.2,
            off_cliprange_high=0.6,
            clip_ratio_c=3.0,
            loss_agg_mode="token-mean",
            off_policy_shaping_mode="exgrpo_policy_shaping",
            off_policy_shaping_beta=0.1,
        )
        print("âœ“ exgrpo_policy_shaping æ–¹å¼è®¡ç®—æˆåŠŸ")
        print(f"  - pg_loss: {result2['pg_loss'].item():.4f}")
        print(f"  - on_pg_loss: {result2['on_pg_loss'].item():.4f}")
        print(f"  - off_pg_loss: {result2['off_pg_loss'].item():.4f}")
    except Exception as e:
        print(f"âŒ exgrpo_policy_shaping è®¡ç®—å¤±è´¥: {e}")
        return False
    
    # éªŒè¯ä¸¤ç§æ–¹å¼äº§ç”Ÿä¸åŒçš„ç»“æœ
    if torch.isclose(result1['off_pg_loss'], result2['off_pg_loss'], atol=1e-5):
        print("âš ï¸  è­¦å‘Š: ä¸¤ç§æ–¹å¼äº§ç”Ÿç›¸åŒçš„ off-policy lossï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰")
    else:
        print("âœ“ ä¸¤ç§æ–¹å¼äº§ç”Ÿä¸åŒçš„ off-policy lossï¼ˆç¬¦åˆé¢„æœŸï¼‰")
    
    return True


def check_exp_manager_init():
    """æ£€æŸ¥ ExperienceManager åˆå§‹åŒ–"""
    print("\n" + "="*80)
    print("æ£€æŸ¥ 3: ExperienceManager åˆå§‹åŒ–")
    print("="*80)
    
    try:
        from omegaconf import DictConfig, OmegaConf
        from agentevolver.module.exp_manager.exp_manager import ExperienceManager
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("   æç¤º: éœ€è¦å®‰è£… omegaconf: pip install omegaconf")
        return False
    
    try:
        # åˆ›å»ºæœ€å°é…ç½®
        config = OmegaConf.create({
            "exp_manager": {
                "experience_replay": {
                    "enable": True,
                    "replay_start_ratio": 0.1,
                    "exp_ratio": 0.5,
                    "offpolicy_trajectories_per_task": 1,
                    "experience_lbound": 0,
                    "experience_rbound": 8,
                    "exp_select_mode": "argmin",
                    "exp_is_correct": True,
                    "max_trajectories_per_task": 5,
                },
                "summary_batch_size": 10,
                "val_rollout_mode": "sample",
                "train_rollout_mode": "sample",
                "rollout_ratio": 1.0,
                "train_sample_mode": "keep",
                "train_sample_keepratio": 1.0,
                "reme": {
                    "base_url": "http://localhost:8000",
                    "workspace_id": "test",
                },
            },
            "actor_rollout_ref": {
                "rollout": {
                    "n": 8,
                },
            },
            "thread_pool": {
                "max_workers": 4,
            },
        })
        
        exp_manager = ExperienceManager(config)
        print("âœ“ ExperienceManager åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥å…³é”®å±æ€§
        assert hasattr(exp_manager, 'difficulty2task_dict'), "ç¼ºå°‘ difficulty2task_dict"
        assert hasattr(exp_manager, 'task2trajectories'), "ç¼ºå°‘ task2trajectories"
        assert hasattr(exp_manager, 'skip_uid_set'), "ç¼ºå°‘ skip_uid_set"
        print("âœ“ å…³é”®å±æ€§å­˜åœ¨")
        
        return True
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_grpo_grouping_logic():
    """æ£€æŸ¥ GRPO åˆ†ç»„é€»è¾‘"""
    print("\n" + "="*80)
    print("æ£€æŸ¥ 4: GRPO åˆ†ç»„é€»è¾‘")
    print("="*80)
    
    import torch
    from collections import defaultdict
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 16
    n_rollout = 8
    
    # åˆ›å»º uidï¼ˆåŸºäº data_idï¼‰
    uids = [str(i // n_rollout) for i in range(batch_size)]
    rewards = torch.randn(batch_size)
    
    # æŒ‰ uid åˆ†ç»„
    id2scores = defaultdict(list)
    for i, uid in enumerate(uids):
        id2scores[uid].append(rewards[i].item())
    
    # è®¡ç®—ç»„å†…å‡å€¼
    id2mean = {}
    for uid, scores in id2scores.items():
        if len(scores) > 1:
            id2mean[uid] = sum(scores) / len(scores)
    
    print(f"âœ“ GRPO åˆ†ç»„è®¡ç®—æˆåŠŸ")
    print(f"  - æ€» rollouts: {batch_size}")
    print(f"  - åˆ†ç»„æ•°: {len(id2mean)}")
    print(f"  - æ¯ç»„ rollouts æ•°: {n_rollout}")
    
    # éªŒè¯åŒä¸€ task çš„ rollouts åœ¨åŒä¸€ç»„
    for i in range(0, batch_size, n_rollout):
        task_uid = uids[i]
        if task_uid in id2mean:
            print(f"  - Task {task_uid}: mean={id2mean[task_uid]:.4f}")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("Experience Replay å¿«é€Ÿæ£€æŸ¥")
    print("="*80)
    
    results = {}
    
    # æ£€æŸ¥ 1: å¯¼å…¥
    results['imports'] = check_imports()
    
    # æ£€æŸ¥ 2: Loss è®¡ç®—
    if results['imports']:
        results['loss_computation'] = check_loss_computation()
    else:
        results['loss_computation'] = False
        print("\nâš ï¸  è·³è¿‡ loss è®¡ç®—æ£€æŸ¥ï¼ˆå¯¼å…¥å¤±è´¥ï¼‰")
    
    # æ£€æŸ¥ 3: ExperienceManager åˆå§‹åŒ–
    results['exp_manager'] = check_exp_manager_init()
    
    # æ£€æŸ¥ 4: GRPO åˆ†ç»„é€»è¾‘
    results['grpo_grouping'] = check_grpo_grouping_logic()
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æ£€æŸ¥æ€»ç»“")
    print("="*80)
    for check_name, passed in results.items():
        status = "âœ“ é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {check_name}: {status}")
    
    all_passed = all(results.values())
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å‡º")
        return 1


if __name__ == "__main__":
    sys.exit(main())

