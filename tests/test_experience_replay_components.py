#!/usr/bin/env python3
"""
Experience Replay ç»„ä»¶å¿«é€Ÿæµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬å¯ä»¥å¿«é€Ÿæµ‹è¯• experience replay æµç¨‹çš„å„ä¸ªç»„ä»¶ï¼Œæ— éœ€å®Œæ•´è®­ç»ƒæµç¨‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    python tests/test_experience_replay_components.py [--test TEST_NAME]

æµ‹è¯•é¡¹:
    - all: è¿è¡Œæ‰€æœ‰æµ‹è¯•
    - exp_manager: æµ‹è¯• ExperienceManager åŸºæœ¬åŠŸèƒ½ï¼ˆåˆå§‹åŒ–ã€difficulty2task_dict æ›´æ–°ã€valid_replay_task_idsï¼‰
    - trajectory_storage: æµ‹è¯•è½¨è¿¹å­˜å‚¨å’Œæ£€ç´¢ï¼ˆsave_trajectories_to_memoryã€get_offpolicy_trajectories_from_memoryï¼‰
    - mix_collate: æµ‹è¯• ExperienceMixCollateFnï¼ˆtask æ··åˆã€exp_ratio æ§åˆ¶ï¼‰
    - offpolicy_retrieval: æµ‹è¯• off-policy è½¨è¿¹è·å–ï¼ˆget_offpolicy_batchã€get_all_candidates_batchï¼‰
    - loss_computation: æµ‹è¯• loss è®¡ç®—ï¼ˆä¸¤ç§ policy shaping æ–¹å¼ï¼šhigher_clip_boundã€exgrpo_policy_shapingï¼‰
    - grpo_grouping: æµ‹è¯• GRPO åˆ†ç»„æœºåˆ¶ï¼ˆExperience Replay åœºæ™¯ï¼šon-policy + off-policy æ··åˆåˆ†ç»„ï¼‰
    - skip_uid_set: æµ‹è¯• skip_uid_set æ›´æ–°é€»è¾‘ï¼ˆå…¨å¯¹/éƒ¨åˆ†æˆåŠŸ/å…¨å¤±è´¥åœºæ™¯ï¼‰
    - e2e: ç«¯åˆ°ç«¯æµ‹è¯•ï¼ˆæ¨¡æ‹Ÿå®Œæ•´ Experience Replay æµç¨‹ï¼‰
"""

import sys
import os
import argparse
import numpy as np
import torch
from typing import List, Dict
from collections import defaultdict

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from omegaconf import DictConfig, OmegaConf
from agentevolver.schema.task import Task
from agentevolver.schema.trajectory import Trajectory, Reward
from agentevolver.module.exp_manager.exp_manager import ExperienceManager
from agentevolver.module.exp_manager.experience_collate import ExperienceMixCollateFn
from agentevolver.module.exp_manager.het_core_algos import het_compute_token_on_off_policy_loss


# ============================================================================
# æµ‹è¯•æ•°æ®ç”Ÿæˆ
# ============================================================================

def create_mock_config() -> DictConfig:
    """åˆ›å»ºæ¨¡æ‹Ÿé…ç½®"""
    config = OmegaConf.create({
        "exp_manager": {
            "experience_replay": {
                "enable": True,
                "replay_start_ratio": 0.1,
                "exp_ratio": 0.5,
                "offpolicy_trajectories_per_task": 1,
                "experience_lbound": 0,
                "experience_rbound": 8,
                "exp_select_mode": "argmin",
                "exp_is_correct": True,
                "max_trajectories_per_task": 5,
                "use_current_policy_entropy": False,
            },
            "summary_batch_size": 10,
            "val_rollout_mode": "sample",
            "train_rollout_mode": "sample",
            "rollout_ratio": 1.0,
            "train_sample_mode": "keep",
            "train_sample_keepratio": 1.0,
            "reme": {
                "base_url": "http://localhost:8000",
                "workspace_id": "test",
            },
        },
        "actor_rollout_ref": {
            "rollout": {
                "n": 8,
            },
        },
        "thread_pool": {
            "max_workers": 4,
        },
    })
    return config


def create_mock_task(task_id: str, query: str = None) -> Task:
    """åˆ›å»ºæ¨¡æ‹Ÿ Task"""
    return Task(
        task_id=task_id,
        env_type="appworld",
        open_query=False,
        query=query or f"Task {task_id}",
        metadata={},
    )


def create_mock_trajectory(
    task_id: str,
    rollout_id: str,
    success: bool = True,
    old_log_probs: List[float] = None,
    entropy: float = None,
) -> Trajectory:
    """åˆ›å»ºæ¨¡æ‹Ÿ Trajectory"""
    steps = [
        {"role": "user", "content": f"Query for task {task_id}"},
        {"role": "assistant", "content": f"Response for task {task_id}, rollout {rollout_id}"},
    ]
    
    reward = Reward(
        outcome=1.0 if success else 0.0,
        success_rate=1.0 if success else 0.0,
    )
    
    metadata = {
        "old_log_probs": old_log_probs or [-0.5] * 10,
        "response_mask": [1] * 10,
        "policy_version": 100,
    }
    
    if entropy is not None:
        metadata["entropy"] = entropy
    
    return Trajectory(
        data_id=task_id,
        rollout_id=rollout_id,
        steps=steps,
        query=f"Task {task_id}",
        is_terminated=True,
        reward=reward,
        metadata=metadata,
    )


def create_mock_task_manager(tasks: List[Task]):
    """åˆ›å»ºæ¨¡æ‹Ÿ TaskManager"""
    class MockTaskManager:
        def __init__(self, tasks):
            self.tasks = {task.task_id: task for task in tasks}
            # _get_task_by_id ä¼šæŸ¥æ‰¾ _tasks å±æ€§
            self._tasks = tasks
        
        def get_task_by_id(self, task_id: str) -> Task:
            return self.tasks.get(task_id)
    
    return MockTaskManager(tasks)


# ============================================================================
# æµ‹è¯•å‡½æ•°
# ============================================================================

def test_exp_manager_basic():
    """æµ‹è¯• ExperienceManager åŸºæœ¬åŠŸèƒ½"""
    print("\n" + "="*80)
    print("æµ‹è¯• 1: ExperienceManager åŸºæœ¬åŠŸèƒ½")
    print("="*80)
    
    config = create_mock_config()
    exp_manager = ExperienceManager(config)
    
    # æ£€æŸ¥åˆå§‹åŒ–
    assert hasattr(exp_manager, 'difficulty2task_dict')
    assert hasattr(exp_manager, 'task2trajectories')
    assert hasattr(exp_manager, 'skip_uid_set')
    print("âœ“ ExperienceManager åˆå§‹åŒ–æˆåŠŸ")
    
    # æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®è¯»å–
    assert exp_manager.replay_start_ratio == 0.1
    assert exp_manager.max_trajectories_per_task == 5
    print("âœ“ é…ç½®è¯»å–æ­£ç¡®")
    
    # æµ‹è¯• update_difficulty2task_dict
    tasks = [create_mock_task(f"task_{i}") for i in range(3)]
    trajectories = []
    for i, task in enumerate(tasks):
        for j in range(8):
            success = (i + j) % 3 == 0  # éƒ¨åˆ†æˆåŠŸ
            traj = create_mock_trajectory(task.task_id, f"rollout_{j}", success=success)
            traj.task_id = task.task_id
            trajectories.append(traj)
    
    exp_manager.update_difficulty2task_dict(trajectories)
    
    print(f"âœ“ difficulty2task_dict æ›´æ–°æˆåŠŸ")
    print(f"  - éš¾åº¦åˆ†å¸ƒ: {dict(exp_manager.difficulty2task_dict)}")
    
    # éªŒè¯æ¯ä¸ª task è¢«åˆ†åˆ°æ­£ç¡®çš„éš¾åº¦æ¡¶
    # task_0: j=0,3,6 æˆåŠŸ (3ä¸ª)
    # task_1: j=2,5 æˆåŠŸ (2ä¸ª) 
    # task_2: j=1,4,7 æˆåŠŸ (3ä¸ª)
    assert len(exp_manager.difficulty2task_dict) > 0, "åº”è¯¥æœ‰éš¾åº¦åˆ†ç»„"
    
    # æµ‹è¯• get_valid_replay_task_idsï¼ˆæ­¤æ—¶ task2trajectories ä¸ºç©ºï¼Œåº”è¯¥è¿”å›ç©ºåˆ—è¡¨ï¼‰
    valid_ids = exp_manager.get_valid_replay_task_ids()
    print(f"âœ“ æœ‰æ•ˆ replay task IDsï¼ˆæ— è½¨è¿¹æ—¶ï¼‰: {len(valid_ids)} ä¸ª")
    assert len(valid_ids) == 0, "task2trajectories ä¸ºç©ºæ—¶ï¼Œvalid_ids åº”ä¸ºç©º"
    
    # ä¸º task_0 æ·»åŠ è½¨è¿¹åˆ° task2trajectories
    task_0_trajs = [t for t in trajectories if t.task_id == "task_0" and t.reward.outcome == 1.0]
    exp_manager.save_trajectories_to_memory(task_0_trajs)
    
    # å†æ¬¡æµ‹è¯• get_valid_replay_task_ids
    valid_ids = exp_manager.get_valid_replay_task_ids()
    print(f"âœ“ æœ‰æ•ˆ replay task IDsï¼ˆæœ‰è½¨è¿¹åï¼‰: {len(valid_ids)} ä¸ª")
    print(f"  - ç¤ºä¾‹: {valid_ids[:3] if valid_ids else 'None'}")
    assert "task_0" in valid_ids, "task_0 åº”è¯¥åœ¨ valid_ids ä¸­"
    
    return True


def test_trajectory_storage():
    """æµ‹è¯•è½¨è¿¹å­˜å‚¨å’Œæ£€ç´¢"""
    print("\n" + "="*80)
    print("æµ‹è¯• 2: è½¨è¿¹å­˜å‚¨å’Œæ£€ç´¢")
    print("="*80)
    
    config = create_mock_config()
    exp_manager = ExperienceManager(config)
    
    # åˆ›å»ºæµ‹è¯•è½¨è¿¹
    task_id = "test_task_1"
    trajectories = []
    for i in range(5):
        traj = create_mock_trajectory(
            task_id, 
            f"rollout_{i}", 
            success=True,
            entropy=0.5 + i * 0.1,  # ä¸åŒçš„ entropy
        )
        traj.task_id = task_id
        trajectories.append(traj)
    
    # æµ‹è¯•ä¿å­˜
    exp_manager.save_trajectories_to_memory(trajectories)
    print(f"âœ“ ä¿å­˜äº† {len(trajectories)} æ¡è½¨è¿¹åˆ° task {task_id}")
    
    # æµ‹è¯•æ£€ç´¢
    retrieved = exp_manager.get_offpolicy_trajectories_from_memory(
        task_id, 
        num_trajectories=1,
        use_saved_entropy=True,
    )
    print(f"âœ“ æ£€ç´¢åˆ° {len(retrieved)} æ¡è½¨è¿¹")
    
    # éªŒè¯é€‰æ‹©çš„æ˜¯ entropy æœ€ä½çš„
    if len(retrieved) > 0:
        entropies = [t.metadata.get("entropy", float('inf')) for t in retrieved]
        print(f"  - æ£€ç´¢åˆ°çš„ entropy: {entropies}")
        assert all(e == min(entropies) for e in entropies), "åº”è¯¥é€‰æ‹© entropy æœ€ä½çš„"
        print("  âœ“ æ­£ç¡®é€‰æ‹©äº† entropy æœ€ä½çš„è½¨è¿¹")
    
    # æµ‹è¯• max_trajectories_per_task é™åˆ¶
    assert len(exp_manager.task2trajectories[task_id]) <= config.exp_manager.experience_replay.max_trajectories_per_task
    print(f"âœ“ è½¨è¿¹æ•°é‡é™åˆ¶æ­£ç¡®: {len(exp_manager.task2trajectories[task_id])} <= {config.exp_manager.experience_replay.max_trajectories_per_task}")
    
    return True


def test_mix_collate():
    """æµ‹è¯• ExperienceMixCollateFn"""
    print("\n" + "="*80)
    print("æµ‹è¯• 3: ExperienceMixCollateFn")
    print("="*80)
    
    config = create_mock_config()
    exp_manager = ExperienceManager(config)
    
    # åˆ›å»ºä¸€äº›ä»»åŠ¡å¹¶æ·»åŠ åˆ° difficulty2task_dict
    exp_tasks = [create_mock_task(f"exp_task_{i}") for i in range(5)]
    for task in exp_tasks:
        exp_manager.difficulty2task_dict[2].append(task.task_id)  # éš¾åº¦ 2
    
    # ä¸º exp_tasks åˆ›å»ºå¹¶ä¿å­˜è½¨è¿¹åˆ° task2trajectories
    # è¿™æ · get_valid_replay_task_ids() æ‰èƒ½è¿”å›æœ‰æ•ˆçš„ task_ids
    for task in exp_tasks:
        trajectories = []
        for j in range(3):  # æ¯ä¸ªä»»åŠ¡åˆ›å»º 3 æ¡è½¨è¿¹
            traj = create_mock_trajectory(
                task.task_id,
                f"rollout_{j}",
                success=True,
                entropy=0.3 + j * 0.1,
            )
            traj.task_id = task.task_id
            trajectories.append(traj)
        exp_manager.save_trajectories_to_memory(trajectories)
    
    print(f"âœ“ ä¸º {len(exp_tasks)} ä¸ª exp_tasks åˆ›å»ºå¹¶ä¿å­˜äº†è½¨è¿¹")
    
    # éªŒè¯ valid_replay_task_ids ä¸ä¸ºç©º
    valid_ids = exp_manager.get_valid_replay_task_ids()
    print(f"âœ“ æœ‰æ•ˆ replay task IDs: {len(valid_ids)} ä¸ª")
    assert len(valid_ids) > 0, "åº”è¯¥æœ‰æœ‰æ•ˆçš„ replay task IDs"
    
    # åˆ›å»ºè®­ç»ƒä»»åŠ¡
    training_tasks = [create_mock_task(f"train_task_{i}") for i in range(10)]
    
    # åˆ›å»ºæ¨¡æ‹Ÿ TaskManager
    all_tasks = exp_tasks + training_tasks
    mock_task_manager = create_mock_task_manager(all_tasks)
    
    # åˆ›å»º ExperienceMixCollateFn
    mix_collate = ExperienceMixCollateFn(
        exp_manager=exp_manager,
        train_task_manager=mock_task_manager,
        exp_ratio=0.5,
        replay_start_ratio=0.1,
        offpolicy_trajectories_per_task=1,
        n_rollout=8,
    )
    
    # æµ‹è¯•æ··åˆ
    experience_tasks, on_policy_tasks = mix_collate(
        training_tasks=training_tasks,
        training_progress=0.5,  # 50% è¿›åº¦ï¼Œåº”è¯¥å¯ç”¨ replay
        enable_replay=True,
    )
    
    print(f"âœ“ æ··åˆæˆåŠŸ")
    print(f"  - Experience tasks: {len(experience_tasks)}")
    print(f"  - On-policy tasks: {len(on_policy_tasks)}")
    print(f"  - æ€»æ•°: {len(experience_tasks) + len(on_policy_tasks)}")
    
    # éªŒè¯æ¯”ä¾‹
    total = len(experience_tasks) + len(on_policy_tasks)
    if total > 0:
        exp_ratio_actual = len(experience_tasks) / total
        print(f"  - å®é™… exp_ratio: {exp_ratio_actual:.2f}")
    
    return True


def test_offpolicy_retrieval():
    """æµ‹è¯• off-policy è½¨è¿¹è·å–"""
    print("\n" + "="*80)
    print("æµ‹è¯• 4: Off-policy è½¨è¿¹è·å–")
    print("="*80)
    
    config = create_mock_config()
    exp_manager = ExperienceManager(config)
    
    # åˆ›å»ºä»»åŠ¡å’Œè½¨è¿¹
    tasks = [create_mock_task(f"task_{i}") for i in range(3)]
    
    for task in tasks:
        # ä¸ºæ¯ä¸ªä»»åŠ¡ä¿å­˜å¤šæ¡è½¨è¿¹
        trajectories = []
        for j in range(3):
            traj = create_mock_trajectory(
                task.task_id,
                f"rollout_{j}",
                success=True,
                entropy=0.3 + j * 0.2,
            )
            traj.task_id = task.task_id
            trajectories.append(traj)
        exp_manager.save_trajectories_to_memory(trajectories)
    
    # æµ‹è¯• get_offpolicy_batch
    offpolicy_trajectories = exp_manager.get_offpolicy_batch(
        tasks=tasks,
        num_trajectories_per_task=1,
    )
    
    print(f"âœ“ è·å–åˆ° {len(offpolicy_trajectories)} æ¡ off-policy è½¨è¿¹")
    for traj in offpolicy_trajectories:
        print(f"  - Task {traj.task_id}: entropy={traj.metadata.get('entropy', 'N/A')}")
    
    # æµ‹è¯• get_all_candidates_batch
    all_candidates = exp_manager.get_all_candidates_batch(tasks=tasks)
    print(f"âœ“ è·å–åˆ°æ‰€æœ‰å€™é€‰è½¨è¿¹")
    for task_id, candidates in all_candidates.items():
        print(f"  - Task {task_id}: {len(candidates)} æ¡å€™é€‰è½¨è¿¹")
    
    return True


def test_loss_computation():
    """æµ‹è¯• loss è®¡ç®—ï¼ˆä¸¤ç§ policy shaping æ–¹å¼ï¼‰"""
    print("\n" + "="*80)
    print("æµ‹è¯• 5: Loss è®¡ç®—ï¼ˆä¸¤ç§ Policy Shaping æ–¹å¼ï¼‰")
    print("="*80)
    
    batch_size, seq_len = 4, 20
    response_len = 10
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    old_log_prob = torch.randn(batch_size, response_len)
    log_prob = torch.randn(batch_size, response_len)
    advantages = torch.randn(batch_size, response_len)
    response_mask = torch.ones(batch_size, response_len)
    exp_mask = torch.zeros(batch_size, response_len)
    exp_mask[0, :] = 1  # ç¬¬ä¸€ä¸ªæ ·æœ¬æ˜¯ off-policy
    exp_mask[1, :] = 1  # ç¬¬äºŒä¸ªæ ·æœ¬ä¹Ÿæ˜¯ off-policy
    
    # æµ‹è¯• higher_clip_bound æ–¹å¼
    print("\næµ‹è¯• higher_clip_bound æ–¹å¼:")
    result1 = het_compute_token_on_off_policy_loss(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        exp_mask=exp_mask,
        cliprange=0.2,
        cliprange_low=0.2,
        cliprange_high=0.2,
        off_cliprange_high=0.6,
        clip_ratio_c=3.0,
        loss_agg_mode="token-mean",
        off_policy_shaping_mode="higher_clip_bound",
        off_policy_shaping_beta=0.1,
    )
    print(f"âœ“ higher_clip_bound è®¡ç®—æˆåŠŸ")
    print(f"  - pg_loss: {result1['pg_loss'].item():.4f}")
    print(f"  - on_pg_loss: {result1['on_pg_loss'].item():.4f}")
    print(f"  - off_pg_loss: {result1['off_pg_loss'].item():.4f}")
    print(f"  - on_pg_clipfrac: {result1['on_pg_clipfrac'].item():.4f}")
    
    # æµ‹è¯• exgrpo_policy_shaping æ–¹å¼
    print("\næµ‹è¯• exgrpo_policy_shaping æ–¹å¼:")
    result2 = het_compute_token_on_off_policy_loss(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        exp_mask=exp_mask,
        cliprange=0.2,
        cliprange_low=0.2,
        cliprange_high=0.2,
        off_cliprange_high=0.6,
        clip_ratio_c=3.0,
        loss_agg_mode="token-mean",
        off_policy_shaping_mode="exgrpo_policy_shaping",
        off_policy_shaping_beta=0.1,
    )
    print(f"âœ“ exgrpo_policy_shaping è®¡ç®—æˆåŠŸ")
    print(f"  - pg_loss: {result2['pg_loss'].item():.4f}")
    print(f"  - on_pg_loss: {result2['on_pg_loss'].item():.4f}")
    print(f"  - off_pg_loss: {result2['off_pg_loss'].item():.4f}")
    print(f"  - on_pg_clipfrac: {result2['on_pg_clipfrac'].item():.4f}")
    
    # éªŒè¯ä¸¤ç§æ–¹å¼äº§ç”Ÿä¸åŒçš„ç»“æœ
    assert not torch.isclose(result1['off_pg_loss'], result2['off_pg_loss'], atol=1e-5), \
        "ä¸¤ç§æ–¹å¼åº”è¯¥äº§ç”Ÿä¸åŒçš„ off-policy loss"
    print("âœ“ ä¸¤ç§æ–¹å¼äº§ç”Ÿä¸åŒçš„ off-policy lossï¼ˆç¬¦åˆé¢„æœŸï¼‰")
    
    # é¢å¤–æµ‹è¯• 1: éªŒè¯ on-policy loss åªä½¿ç”¨ exp_mask=0 çš„æ ·æœ¬
    print("\néªŒè¯ on-policy loss åªä½¿ç”¨ exp_mask=0 çš„æ ·æœ¬:")
    on_policy_mask = (1.0 - exp_mask) * response_mask
    assert on_policy_mask[0].sum() == 0, "ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆoff-policyï¼‰ä¸åº”å‚ä¸ on_pg_loss"
    assert on_policy_mask[1].sum() == 0, "ç¬¬äºŒä¸ªæ ·æœ¬ï¼ˆoff-policyï¼‰ä¸åº”å‚ä¸ on_pg_loss"
    assert on_policy_mask[2].sum() == response_len, "ç¬¬ä¸‰ä¸ªæ ·æœ¬ï¼ˆon-policyï¼‰åº”å‚ä¸ on_pg_loss"
    assert on_policy_mask[3].sum() == response_len, "ç¬¬å››ä¸ªæ ·æœ¬ï¼ˆon-policyï¼‰åº”å‚ä¸ on_pg_loss"
    print("  âœ“ on-policy mask æ­£ç¡®")
    
    # é¢å¤–æµ‹è¯• 2: éªŒè¯ off-policy loss åªä½¿ç”¨ exp_mask=1 çš„æ ·æœ¬
    print("\néªŒè¯ off-policy loss åªä½¿ç”¨ exp_mask=1 çš„æ ·æœ¬:")
    off_policy_mask = exp_mask * response_mask
    assert off_policy_mask[0].sum() == response_len, "ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆoff-policyï¼‰åº”å‚ä¸ off_pg_loss"
    assert off_policy_mask[1].sum() == response_len, "ç¬¬äºŒä¸ªæ ·æœ¬ï¼ˆoff-policyï¼‰åº”å‚ä¸ off_pg_loss"
    assert off_policy_mask[2].sum() == 0, "ç¬¬ä¸‰ä¸ªæ ·æœ¬ï¼ˆon-policyï¼‰ä¸åº”å‚ä¸ off_pg_loss"
    assert off_policy_mask[3].sum() == 0, "ç¬¬å››ä¸ªæ ·æœ¬ï¼ˆon-policyï¼‰ä¸åº”å‚ä¸ off_pg_loss"
    print("  âœ“ off-policy mask æ­£ç¡®")
    
    # é¢å¤–æµ‹è¯• 3: è¾¹ç•Œæƒ…å†µ - å…¨éƒ¨ on-policy
    print("\næµ‹è¯•è¾¹ç•Œæƒ…å†µ - å…¨éƒ¨ on-policy:")
    exp_mask_all_on = torch.zeros(batch_size, response_len)
    result_all_on = het_compute_token_on_off_policy_loss(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        exp_mask=exp_mask_all_on,
        cliprange=0.2,
        cliprange_low=0.2,
        cliprange_high=0.2,
        off_cliprange_high=0.6,
        clip_ratio_c=3.0,
        loss_agg_mode="token-mean",
        off_policy_shaping_mode="higher_clip_bound",
        off_policy_shaping_beta=0.1,
    )
    assert result_all_on['off_pg_loss'].item() == 0.0 or torch.isnan(result_all_on['off_pg_loss']).item(), \
        "å…¨éƒ¨ on-policy æ—¶ï¼Œoff_pg_loss åº”ä¸º 0 æˆ– nan"
    print(f"  âœ“ å…¨éƒ¨ on-policy æ—¶ off_pg_loss = {result_all_on['off_pg_loss'].item():.4f}")
    
    return True


def test_grpo_grouping():
    """
    æµ‹è¯• GRPO åˆ†ç»„æœºåˆ¶ - Experience Replay åœºæ™¯
    
    åœ¨ experience replay åœºæ™¯ä¸‹:
    - åŒä¸€ä¸ª task å¯èƒ½æœ‰ on-policy rollouts å’Œ off-policy rollouts
    - å®ƒä»¬å…±äº«åŒä¸€ä¸ª uidï¼ˆåŸºäº group_ids/data_idï¼‰
    - GRPO è®¡ç®— advantage æ—¶ï¼Œä¼šå°†åŒä¸€ uid çš„æ‰€æœ‰ rollouts åˆ†åˆ°åŒä¸€ç»„
    - exp_mask ç”¨äºåŒºåˆ† on-policy (0) å’Œ off-policy (1)
    """
    print("\n" + "="*80)
    print("æµ‹è¯• 6: GRPO åˆ†ç»„æœºåˆ¶ï¼ˆExperience Replay åœºæ™¯ï¼‰")
    print("="*80)
    
    # ===========================
    # åœºæ™¯è®¾ç½®ï¼šæ¨¡æ‹Ÿæ··åˆ batch
    # ===========================
    # - 2 ä¸ª tasks
    # - æ¯ä¸ª task æœ‰ 6 ä¸ª on-policy rollouts + 2 ä¸ª off-policy rollouts = 8 total
    # - åŒä¸€ task çš„æ‰€æœ‰ rollouts å…±äº«åŒä¸€ä¸ª uid
    n_tasks = 2
    n_on_policy_per_task = 6
    n_off_policy_per_task = 2
    n_total_per_task = n_on_policy_per_task + n_off_policy_per_task
    response_len = 10
    
    batch_size = n_tasks * n_total_per_task  # 2 * 8 = 16
    
    # æ„å»º uid å’Œ exp_mask
    uids = []
    exp_mask = torch.zeros(batch_size, response_len)
    task_ids = []
    is_offpolicy = []
    
    idx = 0
    for task_idx in range(n_tasks):
        task_id = f"task_{task_idx}"
        uid = str(task_idx)  # åŒä¸€ task çš„æ‰€æœ‰ rollouts å…±äº« uid
        
        # On-policy rollouts
        for _ in range(n_on_policy_per_task):
            uids.append(uid)
            task_ids.append(task_id)
            is_offpolicy.append(False)
            # exp_mask é»˜è®¤ä¸º 0ï¼Œè¡¨ç¤º on-policy
            idx += 1
        
        # Off-policy rollouts (experience replay)
        for _ in range(n_off_policy_per_task):
            uids.append(uid)
            task_ids.append(task_id)
            is_offpolicy.append(True)
            exp_mask[idx, :] = 1  # æ ‡è®°ä¸º off-policy
            idx += 1
    
    uids = np.array(uids, dtype=object)
    
    print(f"âœ“ æ„å»ºæ··åˆ batch æˆåŠŸ")
    print(f"  - æ€» rollouts: {batch_size}")
    print(f"  - Tasks æ•°é‡: {n_tasks}")
    print(f"  - æ¯ task on-policy: {n_on_policy_per_task}")
    print(f"  - æ¯ task off-policy: {n_off_policy_per_task}")
    
    # ===========================
    # æ¨¡æ‹Ÿ reward
    # ===========================
    # On-policy: éšæœº reward
    # Off-policy: å†å²æˆåŠŸè½¨è¿¹ï¼Œreward = 1.0
    rewards = torch.zeros(batch_size)
    for i in range(batch_size):
        if is_offpolicy[i]:
            rewards[i] = 1.0  # off-policy æ˜¯å†å²æˆåŠŸè½¨è¿¹
        else:
            rewards[i] = torch.rand(1).item()  # on-policy éšæœº
    
    # ===========================
    # GRPO åˆ†ç»„è®¡ç®— (æ¨¡æ‹Ÿ compute_grpo_outcome_advantage)
    # ===========================
    print("\næ¨¡æ‹Ÿ GRPO åˆ†ç»„è®¡ç®—:")
    
    id2score = defaultdict(list)
    id2mean = {}
    id2std = {}
    
    # åˆ†ç»„
    for i in range(batch_size):
        id2score[uids[i]].append(rewards[i])
    
    # è®¡ç®—æ¯ç»„çš„å‡å€¼å’Œæ ‡å‡†å·®
    for uid in id2score:
        scores = id2score[uid]
        if len(scores) == 1:
            id2mean[uid] = torch.tensor(0.0)
            id2std[uid] = torch.tensor(1.0)
        elif len(scores) > 1:
            id2mean[uid] = torch.mean(torch.stack(scores))
            id2std[uid] = torch.std(torch.stack(scores))
        print(f"  - uid={uid}: {len(scores)} rollouts, mean={id2mean[uid].item():.4f}, std={id2std[uid].item():.4f}")
    
    # è®¡ç®— advantage
    epsilon = 1e-6
    advantages = torch.zeros(batch_size)
    for i in range(batch_size):
        advantages[i] = (rewards[i] - id2mean[uids[i]]) / (id2std[uids[i]] + epsilon)
    
    print(f"\nâœ“ GRPO åˆ†ç»„è®¡ç®—æˆåŠŸ")
    print(f"  - åˆ†ç»„æ•°: {len(id2mean)}")
    
    # ===========================
    # éªŒè¯åˆ†ç»„æ­£ç¡®æ€§
    # ===========================
    print("\néªŒè¯åˆ†ç»„æ­£ç¡®æ€§:")
    
    # éªŒè¯ 1: åŒä¸€ task çš„æ‰€æœ‰ rolloutsï¼ˆon + offï¼‰åœ¨åŒä¸€ç»„
    for task_idx in range(n_tasks):
        uid = str(task_idx)
        task_rollout_indices = [i for i, u in enumerate(uids) if u == uid]
        assert len(task_rollout_indices) == n_total_per_task, \
            f"Task {task_idx} åº”æœ‰ {n_total_per_task} ä¸ª rollouts åœ¨ç»„ {uid}"
        
        # éªŒè¯ on-policy å’Œ off-policy æ•°é‡
        on_policy_count = sum(1 for i in task_rollout_indices if not is_offpolicy[i])
        off_policy_count = sum(1 for i in task_rollout_indices if is_offpolicy[i])
        assert on_policy_count == n_on_policy_per_task
        assert off_policy_count == n_off_policy_per_task
        print(f"  âœ“ Task {task_idx} (uid={uid}): {on_policy_count} on-policy + {off_policy_count} off-policy")
    
    # éªŒè¯ 2: exp_mask æ­£ç¡®æ ‡è®° off-policy
    for i in range(batch_size):
        if is_offpolicy[i]:
            assert exp_mask[i, 0].item() == 1, f"æ ·æœ¬ {i} æ˜¯ off-policyï¼Œexp_mask åº”ä¸º 1"
        else:
            assert exp_mask[i, 0].item() == 0, f"æ ·æœ¬ {i} æ˜¯ on-policyï¼Œexp_mask åº”ä¸º 0"
    print(f"  âœ“ exp_mask æ­£ç¡®æ ‡è®° on/off-policy")
    
    # éªŒè¯ 3: off-policy æ ·æœ¬ï¼ˆå†å²æˆåŠŸè½¨è¿¹ï¼‰çš„ advantage è®¡ç®—
    # off-policy reward = 1.0ï¼Œé€šå¸¸é«˜äºç»„å‡å€¼ï¼Œæ‰€ä»¥ advantage > 0
    for i in range(batch_size):
        if is_offpolicy[i]:
            # ç”±äº off-policy reward = 1.0ï¼Œè€Œç»„å†…æœ‰éšæœº rewardï¼Œ
            # off-policy çš„ advantage é€šå¸¸ä¸ºæ­£ï¼ˆé¼“åŠ±æ¨¡ä»¿æˆåŠŸè½¨è¿¹ï¼‰
            print(f"  - æ ·æœ¬ {i} (off-policy, task={task_ids[i]}): "
                  f"reward={rewards[i].item():.2f}, advantage={advantages[i].item():.4f}")
    
    print("\nâœ“ GRPO åˆ†ç»„æµ‹è¯•é€šè¿‡ï¼ˆExperience Replay åœºæ™¯ï¼‰")
    return True


def test_update_skip_uid_set():
    """æµ‹è¯• skip_uid_set æ›´æ–°é€»è¾‘"""
    print("\n" + "="*80)
    print("æµ‹è¯• 7: skip_uid_set æ›´æ–°é€»è¾‘")
    print("="*80)
    
    config = create_mock_config()
    exp_manager = ExperienceManager(config)
    
    # ===========================
    # åœºæ™¯ 1: å…¨éƒ¨æˆåŠŸçš„ task åº”è¯¥åŠ å…¥ skip_uid_set
    # ===========================
    print("\nåœºæ™¯ 1: å…¨éƒ¨æˆåŠŸçš„ task")
    task_id_full_success = "test_task_full_success"
    trajectories_full_success = []
    for i in range(8):
        traj = create_mock_trajectory(task_id_full_success, f"rollout_{i}", success=True)
        traj.task_id = task_id_full_success
        trajectories_full_success.append(traj)
    
    filtered = exp_manager.update_skip_uid_set_and_filter_trajectories(
        trajectories=trajectories_full_success,
        n_rollout=8,
    )
    
    print(f"  - Task {task_id_full_success} æ˜¯å¦åœ¨ skip_uid_set: {task_id_full_success in exp_manager.skip_uid_set}")
    print(f"  - ç­›é€‰å‡ºçš„è½¨è¿¹æ•°: {len(filtered)}")
    
    assert task_id_full_success in exp_manager.skip_uid_set, "å…¨éƒ¨æˆåŠŸçš„ä»»åŠ¡åº”è¯¥åŠ å…¥ skip_uid_set"
    assert len(filtered) == 0, "å…¨éƒ¨æˆåŠŸçš„ä»»åŠ¡ä¸åº”è¯¥æœ‰ç­›é€‰å‡ºçš„è½¨è¿¹"
    print("  âœ“ å…¨éƒ¨æˆåŠŸçš„ä»»åŠ¡æ­£ç¡®åŠ å…¥ skip_uid_set")
    
    # ===========================
    # åœºæ™¯ 2: éƒ¨åˆ†æˆåŠŸçš„ task åº”è¯¥æœ‰ç­›é€‰å‡ºçš„è½¨è¿¹ï¼ˆéå…¨å¯¹éå…¨é”™ï¼‰
    # ===========================
    print("\nåœºæ™¯ 2: éƒ¨åˆ†æˆåŠŸçš„ taskï¼ˆç¬¦åˆ experience_lbound < success < experience_rboundï¼‰")
    task_id_partial = "test_task_partial_success"
    trajectories_partial = []
    # 3 ä¸ªæˆåŠŸï¼Œ5 ä¸ªå¤±è´¥ (0 < 3 < 8ï¼Œç¬¦åˆæ¡ä»¶)
    for i in range(8):
        success = i < 3
        traj = create_mock_trajectory(task_id_partial, f"rollout_{i}", success=success)
        traj.task_id = task_id_partial
        trajectories_partial.append(traj)
    
    filtered_partial = exp_manager.update_skip_uid_set_and_filter_trajectories(
        trajectories=trajectories_partial,
        n_rollout=8,
    )
    
    print(f"  - Task {task_id_partial} æ˜¯å¦åœ¨ skip_uid_set: {task_id_partial in exp_manager.skip_uid_set}")
    print(f"  - ç­›é€‰å‡ºçš„è½¨è¿¹æ•°: {len(filtered_partial)}")
    
    assert task_id_partial not in exp_manager.skip_uid_set, "éƒ¨åˆ†æˆåŠŸçš„ä»»åŠ¡ä¸åº”è¯¥åœ¨ skip_uid_set"
    assert len(filtered_partial) == 3, "åº”è¯¥ç­›é€‰å‡º 3 æ¡æˆåŠŸè½¨è¿¹"
    print("  âœ“ éƒ¨åˆ†æˆåŠŸçš„ä»»åŠ¡æ­£ç¡®ç­›é€‰è½¨è¿¹")
    
    # ===========================
    # åœºæ™¯ 3: å…¨éƒ¨å¤±è´¥çš„ task ä¸åº”è¯¥æœ‰ç­›é€‰å‡ºçš„è½¨è¿¹
    # ===========================
    print("\nåœºæ™¯ 3: å…¨éƒ¨å¤±è´¥çš„ task")
    task_id_all_fail = "test_task_all_fail"
    trajectories_all_fail = []
    for i in range(8):
        traj = create_mock_trajectory(task_id_all_fail, f"rollout_{i}", success=False)
        traj.task_id = task_id_all_fail
        trajectories_all_fail.append(traj)
    
    filtered_all_fail = exp_manager.update_skip_uid_set_and_filter_trajectories(
        trajectories=trajectories_all_fail,
        n_rollout=8,
    )
    
    print(f"  - Task {task_id_all_fail} æ˜¯å¦åœ¨ skip_uid_set: {task_id_all_fail in exp_manager.skip_uid_set}")
    print(f"  - ç­›é€‰å‡ºçš„è½¨è¿¹æ•°: {len(filtered_all_fail)}")
    
    assert task_id_all_fail not in exp_manager.skip_uid_set, "å…¨éƒ¨å¤±è´¥çš„ä»»åŠ¡ä¸åº”è¯¥åœ¨ skip_uid_set"
    assert len(filtered_all_fail) == 0, "å…¨éƒ¨å¤±è´¥çš„ä»»åŠ¡ä¸åº”è¯¥æœ‰ç­›é€‰å‡ºçš„è½¨è¿¹"
    print("  âœ“ å…¨éƒ¨å¤±è´¥çš„ä»»åŠ¡æ­£ç¡®å¤„ç†")
    
    # ===========================
    # åœºæ™¯ 4: ä¹‹å‰åœ¨ skip_uid_set çš„ task å¦‚æœè¿™æ¬¡æ²¡å…¨å¯¹ï¼Œåº”è¯¥ç§»é™¤
    # ===========================
    print("\nåœºæ™¯ 4: ä» skip_uid_set ä¸­ç§»é™¤")
    # å…ˆç¡®ä¿ task åœ¨ skip_uid_set ä¸­
    assert task_id_full_success in exp_manager.skip_uid_set
    
    # æ¨¡æ‹Ÿè¿™ä¸ª task è¿™æ¬¡æ²¡å…¨å¯¹
    trajectories_not_full = []
    for i in range(8):
        success = i < 5  # 5 ä¸ªæˆåŠŸï¼Œ3 ä¸ªå¤±è´¥
        traj = create_mock_trajectory(task_id_full_success, f"rollout_{i}", success=success)
        traj.task_id = task_id_full_success
        trajectories_not_full.append(traj)
    
    exp_manager.update_skip_uid_set_and_filter_trajectories(
        trajectories=trajectories_not_full,
        n_rollout=8,
    )
    
    print(f"  - Task {task_id_full_success} æ˜¯å¦è¿˜åœ¨ skip_uid_set: {task_id_full_success in exp_manager.skip_uid_set}")
    assert task_id_full_success not in exp_manager.skip_uid_set, "ä¸å†å…¨å¯¹çš„ä»»åŠ¡åº”è¯¥ä» skip_uid_set ç§»é™¤"
    print("  âœ“ ä¸å†å…¨å¯¹çš„ä»»åŠ¡æ­£ç¡®ä» skip_uid_set ç§»é™¤")
    
    print("\nâœ“ skip_uid_set æ›´æ–°é€»è¾‘æµ‹è¯•é€šè¿‡")
    return True


def test_end_to_end():
    """
    ç«¯åˆ°ç«¯æµ‹è¯•ï¼šæ¨¡æ‹Ÿå®Œæ•´çš„ Experience Replay æµç¨‹
    
    æµç¨‹ï¼š
    1. åˆå§‹åŒ– ExperienceManager
    2. æ¨¡æ‹Ÿå¤šä¸ª training stepsï¼Œæ¯ä¸ª step ç”Ÿæˆ trajectories
    3. æ›´æ–° difficulty2task_dict å’Œ task2trajectories
    4. ä½¿ç”¨ ExperienceMixCollateFn æ··åˆ tasks
    5. æ¨¡æ‹Ÿ GRPO åˆ†ç»„å’Œ loss è®¡ç®—
    """
    print("\n" + "="*80)
    print("æµ‹è¯• 8: ç«¯åˆ°ç«¯æµ‹è¯•ï¼ˆå®Œæ•´ Experience Replay æµç¨‹ï¼‰")
    print("="*80)
    
    config = create_mock_config()
    exp_manager = ExperienceManager(config)
    
    # ===========================
    # Step 1: åˆå§‹è®­ç»ƒé˜¶æ®µï¼ˆç§¯ç´¯ç»éªŒï¼‰
    # ===========================
    print("\n=== Step 1: åˆå§‹è®­ç»ƒé˜¶æ®µ ===")
    
    # åˆ›å»º 5 ä¸ªä»»åŠ¡
    all_tasks = [create_mock_task(f"task_{i}") for i in range(5)]
    mock_task_manager = create_mock_task_manager(all_tasks)
    
    # æ¨¡æ‹Ÿç¬¬ä¸€ä¸ª training step
    step1_trajectories = []
    for task in all_tasks:
        for j in range(8):
            # ä¸åŒä»»åŠ¡æœ‰ä¸åŒçš„æˆåŠŸç‡
            task_idx = int(task.task_id.split("_")[1])
            success = j < (3 + task_idx % 3)  # task_0: 3æˆåŠŸ, task_1: 4æˆåŠŸ, task_2: 5æˆåŠŸ, ...
            traj = create_mock_trajectory(
                task.task_id, 
                f"rollout_{j}", 
                success=success,
                entropy=0.5 + j * 0.05,
            )
            traj.task_id = task.task_id
            step1_trajectories.append(traj)
    
    # æ›´æ–° difficulty2task_dict
    exp_manager.update_difficulty2task_dict(step1_trajectories)
    print(f"âœ“ æ›´æ–° difficulty2task_dict: {dict(exp_manager.difficulty2task_dict)}")
    
    # ç­›é€‰å¹¶ä¿å­˜è½¨è¿¹
    filtered = exp_manager.update_skip_uid_set_and_filter_trajectories(
        trajectories=step1_trajectories,
        n_rollout=8,
    )
    exp_manager.save_trajectories_to_memory(filtered)
    print(f"âœ“ ä¿å­˜äº† {len(filtered)} æ¡è½¨è¿¹åˆ° task2trajectories")
    print(f"  - task2trajectories ä¸­çš„ task æ•°: {len(exp_manager.task2trajectories)}")
    
    # ===========================
    # Step 2: Experience Replay é˜¶æ®µ
    # ===========================
    print("\n=== Step 2: Experience Replay é˜¶æ®µ ===")
    
    # æ£€æŸ¥ valid_replay_task_ids
    valid_ids = exp_manager.get_valid_replay_task_ids()
    print(f"âœ“ æœ‰æ•ˆ replay task IDs: {len(valid_ids)} ä¸ª")
    
    if len(valid_ids) > 0:
        # åˆ›å»º ExperienceMixCollateFn
        mix_collate = ExperienceMixCollateFn(
            exp_manager=exp_manager,
            train_task_manager=mock_task_manager,
            exp_ratio=0.5,
            replay_start_ratio=0.1,
            offpolicy_trajectories_per_task=1,
            n_rollout=8,
        )
        
        # æ–°çš„ training tasksï¼ˆè¿™æ¬¡ç”¨å‰ 3 ä¸ªï¼‰
        training_tasks = all_tasks[:3]
        
        # æ··åˆ tasksï¼ˆtraining_progress = 0.5ï¼Œè¶…è¿‡ replay_start_ratioï¼‰
        experience_tasks, on_policy_tasks = mix_collate(
            training_tasks=training_tasks,
            training_progress=0.5,
            enable_replay=True,
        )
        
        print(f"âœ“ æ··åˆæˆåŠŸ")
        print(f"  - Experience tasks: {len(experience_tasks)}")
        print(f"  - On-policy tasks: {len(on_policy_tasks)}")
        
        # éªŒè¯æ€»æ•°ä¸å˜
        assert len(experience_tasks) + len(on_policy_tasks) == len(training_tasks), \
            "æ··åˆåæ€»æ•°åº”è¯¥ç­‰äºåŸå§‹ training_tasks æ•°é‡"
        print("  âœ“ æ€»æ•°éªŒè¯é€šè¿‡")
    
    # ===========================
    # Step 3: æ¨¡æ‹Ÿ GRPO åˆ†ç»„å’Œ Loss è®¡ç®—
    # ===========================
    print("\n=== Step 3: GRPO åˆ†ç»„å’Œ Loss è®¡ç®— ===")
    
    # æ¨¡æ‹Ÿæ··åˆ batch
    batch_size = 16
    response_len = 10
    n_on_policy = 12
    n_off_policy = 4
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    old_log_prob = torch.randn(batch_size, response_len)
    log_prob = torch.randn(batch_size, response_len)
    response_mask = torch.ones(batch_size, response_len)
    
    # exp_mask: å 4 ä¸ªæ ·æœ¬æ˜¯ off-policy
    exp_mask = torch.zeros(batch_size, response_len)
    exp_mask[n_on_policy:, :] = 1
    
    # rewards: on-policy éšæœºï¼Œoff-policy = 1.0
    rewards = torch.zeros(batch_size)
    rewards[:n_on_policy] = torch.rand(n_on_policy)
    rewards[n_on_policy:] = 1.0
    
    # æ¨¡æ‹Ÿ advantages (ç®€åŒ–ç‰ˆ)
    advantages = (rewards - rewards.mean()).unsqueeze(-1).expand(-1, response_len)
    
    # è®¡ç®— loss
    result = het_compute_token_on_off_policy_loss(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        exp_mask=exp_mask,
        cliprange=0.2,
        cliprange_low=0.2,
        cliprange_high=0.2,
        off_cliprange_high=0.6,
        clip_ratio_c=3.0,
        loss_agg_mode="token-mean",
        off_policy_shaping_mode="higher_clip_bound",
        off_policy_shaping_beta=0.1,
    )
    
    print(f"âœ“ Loss è®¡ç®—æˆåŠŸ")
    print(f"  - pg_loss: {result['pg_loss'].item():.4f}")
    print(f"  - on_pg_loss: {result['on_pg_loss'].item():.4f}")
    print(f"  - off_pg_loss: {result['off_pg_loss'].item():.4f}")
    
    # éªŒè¯ on/off-policy loss åˆ†å¼€è®¡ç®—
    assert not torch.isnan(result['on_pg_loss']), "on_pg_loss ä¸åº”ä¸º NaN"
    print("  âœ“ Loss éªŒè¯é€šè¿‡")
    
    # ===========================
    # Step 4: æ¨¡æ‹Ÿä»»åŠ¡å®Œå…¨åšå¯¹åç§»é™¤
    # ===========================
    print("\n=== Step 4: æ¨¡æ‹Ÿä»»åŠ¡å®Œå…¨åšå¯¹ ===")
    
    # æ¨¡æ‹Ÿ task_0 å…¨éƒ¨åšå¯¹
    task_0_full_success = []
    for j in range(8):
        traj = create_mock_trajectory("task_0", f"rollout_{j}", success=True)
        traj.task_id = "task_0"
        task_0_full_success.append(traj)
    
    exp_manager.update_skip_uid_set_and_filter_trajectories(
        trajectories=task_0_full_success,
        n_rollout=8,
    )
    
    print(f"  - task_0 åœ¨ skip_uid_set: {'task_0' in exp_manager.skip_uid_set}")
    print(f"  - task_0 åœ¨ task2trajectories: {'task_0' in exp_manager.task2trajectories}")
    
    if "task_0" in exp_manager.skip_uid_set:
        print("  âœ“ å®Œå…¨åšå¯¹çš„ä»»åŠ¡æ­£ç¡®åŠ å…¥ skip_uid_set")
    
    print("\nâœ“ ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡ï¼")
    return True


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Experience Replay ç»„ä»¶æµ‹è¯•")
    parser.add_argument(
        "--test",
        type=str,
        default="all",
        choices=["all", "exp_manager", "trajectory_storage", "mix_collate", 
                 "offpolicy_retrieval", "loss_computation", "grpo_grouping", "skip_uid_set", "e2e"],
        help="è¦è¿è¡Œçš„æµ‹è¯•",
    )
    args = parser.parse_args()
    
    test_functions = {
        "exp_manager": test_exp_manager_basic,
        "trajectory_storage": test_trajectory_storage,
        "mix_collate": test_mix_collate,
        "offpolicy_retrieval": test_offpolicy_retrieval,
        "loss_computation": test_loss_computation,
        "grpo_grouping": test_grpo_grouping,
        "skip_uid_set": test_update_skip_uid_set,
        "e2e": test_end_to_end,
    }
    
    if args.test == "all":
        tests_to_run = list(test_functions.keys())
    else:
        tests_to_run = [args.test]
    
    print("\n" + "="*80)
    print("Experience Replay ç»„ä»¶æµ‹è¯•")
    print("="*80)
    print(f"è¿è¡Œæµ‹è¯•: {', '.join(tests_to_run)}")
    print("="*80)
    
    results = {}
    for test_name in tests_to_run:
        try:
            results[test_name] = test_functions[test_name]()
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• {test_name} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results[test_name] = False
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“")
    print("="*80)
    for test_name, passed in results.items():
        status = "âœ“ é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    all_passed = all(results.values())
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å‡º")
        return 1


if __name__ == "__main__":
    sys.exit(main())

