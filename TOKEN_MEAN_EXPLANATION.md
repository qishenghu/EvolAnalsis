# Token-Level Policy Gradient: `token-mean` å®ç°è¯¦è§£

## ğŸ“ æ ¸å¿ƒå®ç°ä½ç½®

### **1. é…ç½®è®¾ç½®**

**æ–‡ä»¶**: `config/alfworld_dapo_3b.yaml`

```yaml
actor_rollout_ref:
  actor:
    # â­ Token-Level Policy Gradient (DAPO's 3rd improvement)
    loss_agg_mode: token-mean
```

---

### **2. æ ¸å¿ƒå®ç°å‡½æ•°**

**æ–‡ä»¶**: `agentevolver/module/exp_manager/het_core_algos.py:8-41`

```python
def agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):
    """
    Aggregate the loss matrix into a scalar.
    
    Args:
        loss_mat: shape (bs, response_length) - æ¯ä¸ª token çš„ loss
        loss_mask: shape (bs, response_length) - maskï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆ token
        loss_agg_mode: "token-mean" | "seq-mean-token-sum" | ...
    """
    if loss_agg_mode == "token-mean":
        # â­ å…³é”®ï¼šç›´æ¥å¯¹æ‰€æœ‰æœ‰æ•ˆ token æ±‚å¹³å‡
        loss = verl_F.masked_mean(loss_mat, loss_mask)
    elif loss_agg_mode == "seq-mean-token-sum":
        # å…ˆå¯¹æ¯ä¸ªåºåˆ—æ±‚å’Œï¼Œå†å¯¹åºåˆ—æ±‚å¹³å‡
        seq_losses = torch.sum(loss_mat * loss_mask, dim=-1)  # (bs,)
        loss = torch.mean(seq_losses)  # scalar
    # ... å…¶ä»–æ¨¡å¼
    return loss
```

**`masked_mean` å®ç°** (verl åº“):

```python
def masked_mean(values, mask, axis=None):
    """
    è®¡ç®— masked å…ƒç´ çš„å¹³å‡å€¼
    
    å…¬å¼: sum(values * mask) / (sum(mask) + 1e-8)
    """
    return (values * mask).sum(axis=axis) / (mask.sum(axis=axis) + 1e-8)
```

---

### **3. åœ¨ DAPO Loss è®¡ç®—ä¸­çš„ä½¿ç”¨**

**æ–‡ä»¶**: `agentevolver/module/exp_manager/het_core_algos.py:458`

```python
def dapo_compute_policy_loss(...):
    # ... è®¡ç®—æ¯ä¸ª token çš„ loss (pg_losses)
    # pg_losses shape: (batch_size, response_length)
    # æ¯ä¸ªä½ç½®éƒ½æœ‰ loss å€¼ï¼ˆå¯†é›†çš„ï¼‰
    
    # â­ å…³é”®è°ƒç”¨ï¼šä½¿ç”¨ token-mean èšåˆ
    pg_loss = agg_loss(
        loss_mat=pg_losses,           # (bs, seq_len) - æ¯ä¸ª token çš„ loss
        loss_mask=response_mask,      # (bs, seq_len) - æœ‰æ•ˆ token mask
        loss_agg_mode=loss_agg_mode   # "token-mean"
    )
    
    return {"pg_loss": pg_loss, ...}
```

---

### **4. åœ¨ Actor Update ä¸­çš„è°ƒç”¨é“¾**

**æ–‡ä»¶**: `agentevolver/module/exp_manager/het_actor.py:165-178`

```python
# Step 1: ä»é…ç½®è¯»å– loss_agg_mode
loss_agg_mode = self.config.get("loss_agg_mode", "token-mean")

# Step 2: è°ƒç”¨ DAPO loss è®¡ç®—
ret_dict = dapo_compute_policy_loss(
    old_log_prob=old_log_prob,
    log_prob=log_prob,
    advantages=advantages,
    response_mask=response_mask,
    exp_mask=exp_mask,
    cliprange_low=clip_ratio_low,
    cliprange_high=clip_ratio_high,
    clip_ratio_c=clip_ratio_c,
    loss_agg_mode=loss_agg_mode,  # â­ ä¼ å…¥ "token-mean"
    ...
)

# Step 3: è·å–èšåˆåçš„ loss
pg_loss = ret_dict["pg_loss"]  # scalar
```

---

## ğŸ” å…·ä½“è®¡ç®—è¿‡ç¨‹

### **ç¤ºä¾‹ï¼šç†è§£ `token-mean` vs `seq-mean-token-sum`**

å‡è®¾æœ‰ 3 ä¸ªæ ·æœ¬ï¼š

```python
# è¾“å…¥æ•°æ®
pg_losses = torch.tensor([
    [0.1, 0.2, 0.3, 0.0, 0.0],  # æ ·æœ¬ 1: 3 ä¸ªæœ‰æ•ˆ token
    [0.4, 0.5, 0.0, 0.0, 0.0],  # æ ·æœ¬ 2: 2 ä¸ªæœ‰æ•ˆ token
    [0.6, 0.7, 0.8, 0.9, 1.0],  # æ ·æœ¬ 3: 5 ä¸ªæœ‰æ•ˆ token
])  # shape: (3, 5)

response_mask = torch.tensor([
    [1, 1, 1, 0, 0],  # æ ·æœ¬ 1
    [1, 1, 0, 0, 0],  # æ ·æœ¬ 2
    [1, 1, 1, 1, 1],  # æ ·æœ¬ 3
])  # shape: (3, 5)
```

#### **æ¨¡å¼ 1: `token-mean` (DAPO ä½¿ç”¨)**

```python
# è®¡ç®—è¿‡ç¨‹
total_loss = (pg_losses * response_mask).sum()  # 0.1+0.2+0.3 + 0.4+0.5 + 0.6+0.7+0.8+0.9+1.0 = 4.5
total_tokens = response_mask.sum()  # 3 + 2 + 5 = 10
loss = total_loss / total_tokens  # 4.5 / 10 = 0.45
```

**ç‰¹ç‚¹**:
- âœ… **æ¯ä¸ª token è´¡çŒ®ç›¸ç­‰**ï¼šæ— è®ºæ¥è‡ªå“ªä¸ªåºåˆ—
- âœ… **çŸ­åºåˆ—ä¸ä¼šè¢«è¿‡åº¦åŠ æƒ**ï¼šæ ·æœ¬ 2 åªæœ‰ 2 ä¸ª tokenï¼Œä½†æ¯ä¸ª token çš„æƒé‡å’Œå…¶ä»–æ ·æœ¬ç›¸åŒ
- âœ… **é€‚åˆé•¿æ¨ç†é“¾åœºæ™¯**ï¼šä¸åŒåºåˆ—é•¿åº¦å·®å¼‚å¤§æ—¶ï¼Œé¿å…çŸ­åºåˆ—ä¸»å¯¼è®­ç»ƒ

#### **æ¨¡å¼ 2: `seq-mean-token-sum` (ä¼ ç»Ÿæ–¹å¼)**

```python
# è®¡ç®—è¿‡ç¨‹
seq_losses = (pg_losses * response_mask).sum(dim=-1)  # [0.6, 0.9, 4.0]
loss = seq_losses.mean()  # (0.6 + 0.9 + 4.0) / 3 = 1.83
```

**ç‰¹ç‚¹**:
- âŒ **æ¯ä¸ªåºåˆ—è´¡çŒ®ç›¸ç­‰**ï¼šæ— è®ºåºåˆ—é•¿çŸ­
- âŒ **çŸ­åºåˆ—è¢«è¿‡åº¦åŠ æƒ**ï¼šæ ·æœ¬ 2 åªæœ‰ 2 ä¸ª tokenï¼Œä½†å’Œæ ·æœ¬ 3ï¼ˆ5 ä¸ª tokenï¼‰æƒé‡ç›¸åŒ
- âŒ **ä¸é€‚åˆé•¿æ¨ç†é“¾**ï¼šçŸ­åºåˆ—çš„æ¯ä¸ª token å®é™…ä¸Šè¢«èµ‹äºˆäº†æ›´é«˜çš„æƒé‡

---

## ğŸ“Š æ•°å­¦å…¬å¼å¯¹æ¯”

### **`token-mean` (DAPO)**

```
L = (1 / N_total) * Î£_{i,j} L_{i,j} * mask_{i,j}

å…¶ä¸­:
- N_total = Î£_{i,j} mask_{i,j}  (æ‰€æœ‰æœ‰æ•ˆ token çš„æ€»æ•°)
- L_{i,j}: æ ·æœ¬ i çš„ token j çš„ loss
- mask_{i,j}: 1 è¡¨ç¤ºæœ‰æ•ˆ tokenï¼Œ0 è¡¨ç¤º padding
```

**å«ä¹‰**: æ‰€æœ‰æœ‰æ•ˆ token çš„ loss ç›´æ¥å¹³å‡ï¼Œ**ä¸è€ƒè™‘åºåˆ—è¾¹ç•Œ**ã€‚

### **`seq-mean-token-sum` (ä¼ ç»Ÿ)**

```
L = (1 / N_seq) * Î£_i [Î£_j L_{i,j} * mask_{i,j}]

å…¶ä¸­:
- N_seq: åºåˆ—æ•°é‡ï¼ˆbatch sizeï¼‰
- å…ˆå¯¹æ¯ä¸ªåºåˆ—æ±‚å’Œï¼Œå†å¯¹åºåˆ—æ±‚å¹³å‡
```

**å«ä¹‰**: æ¯ä¸ªåºåˆ—çš„ loss æ€»å’Œå…ˆè®¡ç®—ï¼Œç„¶ååºåˆ—ä¹‹é—´å¹³å‡ï¼Œ**åºåˆ—é•¿åº¦å½±å“æƒé‡**ã€‚

---

## ğŸ¯ ä¸ºä»€ä¹ˆ DAPO ä½¿ç”¨ `token-mean`ï¼Ÿ

### **é—®é¢˜åœºæ™¯**

åœ¨é•¿æ¨ç†é“¾ä»»åŠ¡ï¼ˆå¦‚ AlfWorldï¼‰ä¸­ï¼š
- ä¸åŒæ ·æœ¬çš„å“åº”é•¿åº¦å·®å¼‚å¾ˆå¤§ï¼šçŸ­çš„ 200 tokensï¼Œé•¿çš„ 4000 tokens
- å¦‚æœä½¿ç”¨ `seq-mean-token-sum`ï¼š
  - çŸ­åºåˆ—ï¼ˆ200 tokensï¼‰å’Œé•¿åºåˆ—ï¼ˆ4000 tokensï¼‰çš„ loss æƒé‡ç›¸åŒ
  - ä½†çŸ­åºåˆ—çš„æ¯ä¸ª token å®é™…ä¸Šè¢«èµ‹äºˆäº† **20 å€**çš„æƒé‡ï¼ˆ4000/200ï¼‰
  - è¿™ä¼šå¯¼è‡´æ¨¡å‹åå‘ç”ŸæˆçŸ­å“åº”

### **DAPO çš„è§£å†³æ–¹æ¡ˆ**

ä½¿ç”¨ `token-mean`ï¼š
- æ¯ä¸ª token çš„ loss è´¡çŒ®ç›¸ç­‰
- é•¿åºåˆ—è‡ªç„¶æœ‰æ›´å¤š tokenï¼Œæ‰€ä»¥æ€»è´¡çŒ®æ›´å¤§
- ä½†æ¯ä¸ª token çš„æƒé‡ç›¸åŒï¼Œé¿å…äº†çŸ­åºåˆ—è¢«è¿‡åº¦åŠ æƒ

---

## ğŸ”— å®Œæ•´è°ƒç”¨é“¾

```
é…ç½® (yaml)
  â†“
loss_agg_mode: "token-mean"
  â†“
het_actor.py:174
  â†“ loss_agg_mode å‚æ•°ä¼ é€’
dapo_compute_policy_loss(..., loss_agg_mode="token-mean")
  â†“
het_core_algos.py:458
  â†“
agg_loss(loss_mat=pg_losses, loss_mask=response_mask, loss_agg_mode="token-mean")
  â†“
het_core_algos.py:24
  â†“
verl_F.masked_mean(loss_mat, loss_mask)
  â†“
verl åº“å®ç°: (loss_mat * mask).sum() / (mask.sum() + 1e-8)
  â†“
è¿”å›: scalar loss
```

---

## ğŸ’¡ å…³é”®ä»£ç ä½ç½®ç´¢å¼•

| åŠŸèƒ½ | æ–‡ä»¶ | è¡Œå· |
|------|------|------|
| é…ç½®è®¾ç½® | `config/alfworld_dapo_3b.yaml` | 113 |
| æ ¸å¿ƒå®ç° | `het_core_algos.py` | 8-41 |
| DAPO è°ƒç”¨ | `het_core_algos.py` | 458 |
| Actor è°ƒç”¨ | `het_actor.py` | 174 |
| éªŒè¯æ£€æŸ¥ | `ae_ray_trainer.py` | 749-754 |

---

## âœ… æ€»ç»“

**`token-mean` åœ¨ DAPO ä¸­çš„ä½“ç°**:

1. **é…ç½®å±‚é¢**: `config/alfworld_dapo_3b.yaml:113` è®¾ç½® `loss_agg_mode: token-mean`
2. **å®ç°å±‚é¢**: `het_core_algos.py:24` ä½¿ç”¨ `masked_mean` ç›´æ¥å¯¹æ‰€æœ‰æœ‰æ•ˆ token æ±‚å¹³å‡
3. **è°ƒç”¨å±‚é¢**: `dapo_compute_policy_loss()` åœ¨èšåˆ loss æ—¶è°ƒç”¨ `agg_loss(..., loss_agg_mode="token-mean")`
4. **æ•ˆæœ**: ç¡®ä¿æ¯ä¸ª token å¯¹ loss çš„è´¡çŒ®ç›¸ç­‰ï¼Œé¿å…çŸ­åºåˆ—è¢«è¿‡åº¦åŠ æƒ

è¿™å°±æ˜¯ DAPO çš„ **Token-Level Policy Gradient** æ”¹è¿›çš„æ ¸å¿ƒå®ç°ï¼

